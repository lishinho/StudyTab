1223
1.分页/cookie->不传的话，就是变量对应类的默认初始值，builder模式就是适用于有非常多的变量，但是每一个变量又都不是必填的情况
2.客户端怎么改->就那么改 改成null就行其他不管
3.if语句的异常
private void checkElementIndex(int index) {
        if (!isElementIndex(index))
            throw new IndexOutOfBoundsException(outOfBoundsMsg(index));
    }

builder模式

user类中构造函数来传参数代码可读性很差，调用者很艰难

gettersetter方法 对象可能产生不一致状态，不可变类好处很差

Builder模式使用了链式调用。可读性更佳。
Builder的内部类构造方法中只接收必传的参数，并且该必传的参数适用了final修饰符
Builder模式拥有其所有的优点，而没有上述方法中的缺点。客户端的代码更容易写，并且更重要的是，可读性非常好。唯一可能存在的问题就是会产生多余的Builder对象，消耗内存。然而大多数情况下我们的Builder内部类使用的是静态修饰的(static)，所以这个问题也没多大关系。
由于Builder是非线程安全的，所以如果要在Builder内部类中检查一个参数的合法性，必需要在对象创建完成之后再检查。
建造者模式(Builder Pattern)：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。
public User build() {
  User user = new user(this);
  if (user.getAge() > 120) {
    throw new IllegalStateException(“Age out of range”); // 线程安全
  }
  return user;
}



要好好看之前写的代码
不需要的逻辑不要加

.context.annotation.ConflictingBeanDefinitionException: Annotation-specified bean name 'resourceServiceController' for bean class 
Annotation-specified bean name 'resourceServiceController' for bean class [io.transwarp.guardian.server.boot.controller.ResourceServiceController] conflicts with existing, non-compatible bean definition of same name and class [io.transwarp.guardian.server.boot.controller.v2.ResourceServiceController]

mvn install -DskipTests -Pcopy-deps



du -h --max-depth=1 定位最大文件
cd /var/log 看日志
ls -al
rm messages-201911* -rf

ls –lhS 将文件以从大到小顺序展现

inceptor-server-inceptor1-697b7f4665-zpwzs           0/1       Error                  1          43s       172.16.1.237   tw-node1237

如何删除不需要的分支：  gitlab-repo-branches
setterm -inversescreen on 终端变白
ps 命令虽然在收集运行在系统上的进程信息时非常有用,但也有不足之处:它只能显示某个特定时间点的信息。如果想观察那些频繁换进换出的内存的进程趋势,用 ps 命令就不方便了。
而 top 命令刚好适用这种情况。 top 命令跟 ps 命令相似,能够显示进程信息,但它是实时显示的。

1224
grep

[root@tw-node1237 ~]# history
    1  systemctl status kubelet
    2  systemctl stop kubelet
    3  systemctl status kubelet
    4  systemctl start kubelet
    5  systemctl status kubelet
    6  systemctl status kubelet -l
    7  docker version
    8  docker ps
    9  sysctmctl status docker
   10  systemctl status docker
   11  systemctl restart docker
   12  systemctl status docker.service
   13  systemctl status docker.service -l
   14  ps -efww | grep docker
   15  systemctl status docker.service -l
   16  journalctl -xe
   17  df -h
   18  vi /etc/fstab 
   19  exit
   20   mount /dev/vda /var/lib/docker
   21  df -h
   22  systemctl restart docker.service -l
   23  systemctl status docker.service
   24  kubectl get nodes
   25  kubectl get po
   26  kubectl get po -owide
   27  kubectl logs guardian-apacheds-guardian-847cf7dbbd-42pfh
   28  kubectl get po -owide
   33  blkid /dev/vda
   34  UUID="c2a7c600-3e48-44e5-b3bd-cbac431c3b4e" /var/lib/docker xfs defaults 0 0
   35  vi /etc/fstab
   36  cat /etc/fstab
   37  docker images
   38  docker ps
   39  docker ps |grep inceptor
   40  docker version
   41  docker run -it 844 bash
   42  docker run --help
   43  kubectl get po |grep inceptor
   44  docker images | grep inceptor
   45  docker run it 52f bash
   46  docker run it 968 bash
   47  docker restart
   48  docker run it 52f bash
   49  docker run -it 52f bash
   50  kubectl get po -o wide | grep inceptor
   51  kubectl exec -it inceptor-server-inceptor1-7cdffb54d7-xht9l bash'
   52  kubectl exec -it inceptor-server-inceptor1-7cdffb54d7-xht9l bash
   53  kinit admin
   54  exit
   55  quit
   56  logout
   57  :q
   58  kubectl exec -it inceptor-server-inceptor1-7cdffb54d7-xht9l bash
   59  cat /etc/fstab/
   60  cat /etc/fstab
   61  logout
   62  kubectl get po | grep inceptor
   63  kubectl get po -owide | grep inceptor
   64  kubectl logs -f inceptor-server-inceptor1-697b7f4665-6vc5x
   65  kubectl get po -owide | grep inceptor
   66  kubectl get deploy
   67  kubectl delete deploy inceptor-server-slipstream1
   68  kubectl get deploy
   69  kubectl get po -owide | grep inceptor
   70  kubectl get po -owide |
   71  kubectl get po -owide
   72  docker ps
   73  kubectl get po -owide | grep inceptor
   74  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
   75  jmap
   76  jmap -heap
   77  beeline beeline -u "jdbc:hive2://localhost:10000/default;principal=hive/tw-node1237@TDH"
   78  kinit
   79  kinit admin
   80  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
   81  ls
   82  scp '/home/transwarp/Downloads/guardian-plugins/plugins/inceptor-plugin/target/inceptor-plugin-transwarp-6.2.1.jar' root@172.16.1.237:/root
   83  ls
   84  ls -a
   85  rm inceptor-plugin-transwarp-6.2.1.jar
   86  ls
   87  docker ps
   88  docker ps | grep inceptor
   89  docker ps | grep 968
   90  docker ps | grep bash
   91  docker cp inceptor-plugin-transwarp-6.2.1.jar 844:/root
   92  docker images | grep inceptor
   93  docker commit 844 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
   94  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
   95  kubectl get po | grep inceptor
   96  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
   97  docker ps
   98  kubectl get po | grep inceptor
   99  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x  bash
  100  kubectl get po | grep inceptor
  101  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x  bash
  102  kubectl get po | grep inceptor
  103  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x  bash
  104  ls
  105  rm inceptor-exec-8.0.1.jar
  106  ls
  107  mv inceptor-exec-8.0.1.jar /var
  108  ls
  109  docker ps | grep inceptor
  110  kubectl get po -owide | grep inceptor
  111  docker images |  grep inceptor
  112  pwd
  113  ls /root
  114  ls -al /root
  115  docker images |  grep inceptor
  116  docker run -it 968 bash
  117  ls
  118  docker ps | grep inceptor
  119  docker ps
  120  docker ps | grep 968
  121  docker cp inceptor-exec-8.0.1.jar 157:/root
  122  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  123  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  124  docker images | grep inceptor
  125  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  126  kubectl get po | grep inceptor
  127  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  128  kubectl get po -owide | inceptor
  129  kubectl get po -owide | grep inceptor
  130  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  131  ls
  132  rm -r inceptor-exec-8.0.1.jar
  133  ls
  134  rm -r inceptor-plugin-transwarp-6.2.1.jar
  135  ls
  136  scp '/home/transwarp/Downloads/work/hive-0.12.0-transwarp/src/ql/target/inceptor-exec-8.0.1.jar' root@172.16.1.237:/root
  137  ls
  138  docker ps | grep inceptor
  139  docker ps
  140  docker ps | grep 968
  141  docker cp
  142  docker cp inceptor-exec-8.0.1.jar 157:/root
  143  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  144  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  145  docker images | grep inceptor
  146  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  147  kubectl get po | grep inceptor
  148  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  149  kubectl get po | grep inceptor
  150  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  151  select * from default;
  152  show tables in default;
  153  ls
  154  rm -r inceptor-exec-8.0.1.jar
  155  rm -r inceptor-plugin-transwarp-6.2.1.jar
  156  ls
  157  kubectl get po | grep inceptor
  158  docker ps | grep inceptor
  159  docker ps 
  160  docker ps | grep 968
  161  docker cp inceptor-exec-8.0.1.jar 157:/root
  162  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  163  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  164  docker images | grep inceptor
  165  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  166  kubectl get po -owide | grep inceptor
  167  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  168  ls
  169  rm -r inceptor-plugin-transwarp-6.2.1.jar
  170  rm -r inceptor-exec-8.0.1.jar
  171  ls
  172  logout
  173  ls
  174  docker ps | grep inceptor
  175  docker ps 
  176  docker ps | grep 968
  177  docker cp inceptor-exec-8.0.1.jar 157:/root
  178  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  179  ls
  180  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  181  docker images | grep inceptor
  182  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  183  kubectl get po -owide | grep inceptor
  184  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  185  ls
  186  rm -r inceptor-plugin-transwarp-6.2.1.jar
  187  rm -r inceptor-exec-8.0.1.jar
  188  ls
  189  ls -al /root
  190  ls
  191  docker ps | grep inceptor
  192  docker ps 
  193  docker ps | grep 968
  194  docker cp inceptor-exec-8.0.1.jar 157:/root
  195  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  196  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  197  docker images | grep inceptor
  198  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  199  kubectl get po -owide | grep inceptor
  200  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash


  docker run -it 163 bash
  509  docker images | grep workflow
  510  docker images
  511  docker pull tw-node1236:5000/transwarp/workflow:transwarp-6.2.0-final
  512  docker pull tw-node1236:5000/transwarp/workflow:studio-1.1.0
  513  docker pull 172.16.1.99/gold/workflow:studio-1.1.0-final-2019-08-13-12-30-31-620b62fde11f90e9638091d4c26cc0287e04ae85
  514  docker images | grep workflow
  515  docker images | grep studio
  516  docker imags
  517  docker images
  518  docker run -ti 6b3c8d4b0a07baec433c26f697257b389a2acc5789e1fa866b02075edcb44f35 bash
  519  docker run -it 172.16.1.99/gold/workflow:studio-1.1.0-final-2019-08-13-12-30-31-620b62fde11f90e9638091d4c26cc0287e04ae85 bash
  520  exit
  521  ls
  522  vim /etc/guardian/conf/guardian-site.xml 
  523  vi /etc/guardian/conf/guardian-site.xml 
  524  exit
  525  df -h
  526  cd /var/log
  527  ls -al
  528  rm messages-201911* -rf
  529  ls -al
  530  df -h
  531  kubectl get po -owide
  532  vi /var/log/hdfs1/hadoop-hdfs-namenode-tw-node1237.log
  533  kubectl get po -owide
  534  vi /var/log/inceptor1/hive-server2.log 
  537  vi /var/log/hdfs1
  538  kubectl get po -owide | grep inceptor
命令列界面 CLI
curl -L -H "PRIVATE-TOKEN: swch1r1jLV-fsMe7ZMJn" "http://172.16.1.41:10080/api/v4/projects/402/jobs/artifacts/guardian-3.1/download?job=postcommit" -o artifact.zip
    unzip artifact.zip
    mkdir -p guardian-server-boot/src/main/resources/static
    cp -r public/* guardian-server-boot/src/main/resources/static/

rm artifact.zip -rf
rm public -rf
rm guardian-server-boot/src/main/resources/static -rf
拉取前端页面

bash shell用一个叫作环境变量(environment variable)的特性来存储有关shell会话和工作环境的信息(这也是它们被称作环境变量的原因)。这项特性允许你在内存中存储数据,以便程序或shell中运行的脚本能够轻松访问到它们。这也是存储持久数据的一种简便方法。

命令 env 、 printenv 和 set 之间的差异很细微。 set 命令会显示出全局变量、局部变量以及用户定义变量。它还会按照字母顺序对结果进行排序。 env 和 printenv 命令同 set 命令的区别在于前两个命令不会对变量排序,也不会输出局部变量和用户定义变量。在种情况下, env 和 printenv 的输出是重复的。不过 env 命令有一个 printenv 没有的功能,这使得它要更有用一些

YAML 是专门用来写配置文件的语言，非常简洁和强大，远比 JSON 格式方便。

echo = 可以通过等号给环境变量赋值,值可以是数值或字符串。注意不要加空格 否则会被识别成单独命令
创建全局环境变量的方法是先创建一个局部环境变量,然后再把它导出到全局环境中。这个过程通过 export 命令来完成,变量名前面不需要加 $ 。
可以用 unset 命令完成删除操作。在 unset 命令中引用环境变量时,记住不要使用 $

https://172.26.2.6:8380/
admin/123
QA集群

list范型赋值用的是适配器模式
Hadoop分布式文件系统（HDFS）允许管理员为所使用的名称数量和单个目录使用的空间量设置配额。name quota和space quota独立运作，但这两种配额的管理和实施是相当类似的。


[root@tw-node1237 ~]# ls
anaconda-ks.cfg                         guardian-client-guardian-3.1.3.jar  guardian-core-guardian-3.1.3.jar         inceptor-exec-8.0.1.jar              lib.tar.gz
fortress-core-1.0.0-guardian-3.1.3.jar  guardian-common-guardian-3.1.3.jar  guardian-server-boot-guardian-3.1.3.jar  inceptor-plugin-transwarp-6.2.1.jar  tos.tar.gz

换包

find / -name 'guardian-client*' 查找文件

find . -name "*.jar"| awk '{print "jar -tf "$1}'| sh -x | grep -i "/GuardianClient.class"
hbase-shaded-server-1.3.1-transwarp-6.2.0.jar

https://172.16.1.237:8380/swagger-ui.html#!/perms/findUserDatasourceUsingGET

1225
存在permobj的objName顺序是Table_or_)view->DB->tb
存在permobjdn的string字段中getdn顺序是 tb->db->Table_or_view

2019-12-25 10:04:06,614 ERROR inceptor.GuardianAuthorizer: (GuardianAuthorizer.java:hasAnyTablePrivilegesOnDB(648)) [HiveServer2-Handler-Pool: Thread-237(SessionHandle=d6315725-863a-4db1-b498-4c5b53868b20)] - Fail to filter tables by privileges. username: [test], dbName:[default]

java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
        at java.util.ArrayList.rangeCheck(ArrayList.java:653)
        at java.util.ArrayList.get(ArrayList.java:429)
        at org.apache.directory.fortress.core.impl.PermDAO.getNextLevelObjName(PermDAO.java:3308)
        at org.apache.directory.fortress.core.impl.PermDAO.findUserAuthorizedDataNodes(PermDAO.java:2483)
        at org.apache.directory.fortress.core.impl.PermP.searchDataNodes(PermP.java:220)
        at org.apache.directory.fortress.core.impl.ReviewMgrImpl.userAuthorizedDataNodes(ReviewMgrImpl.java:497)
        at io.transwarp.guardian.core.manager.PermManager.userAuthorizedDataNodes(PermManager.java:183)
        at

docker image rmi id -f

权限控制表 (ACL: Access Control List)
用来描述权限规则或用户和权限之间关系的数据表。
DAC 自主访问控制
系统会识别用户，然后根据被操作对象的ACL或者权限控制矩阵来控制用户权限
如windows的权限
MAC 强制访问控制
MAC是为了弥补DAC权限控制过于分散的问题而诞生的。在MAC的设计中，每一个对象都都有一些权限标识，每个用户同样也会有一些权限标识，而用户能否对该对象进行操作取决于双方的权限标识的关系，这个限制判断通常是由系统硬性限制的。比如在影视作品中我们经常能看到特工在查询机密文件时，屏幕提示需要“无法访问，需要一级安全许可”，这个例子中，文件上就有“一级安全许可”的权限标识，而用户并不具有。
RBAC 基于角色的访问控制
RBAC在用户和权限之间引入了“角色（Role）”的概念（暂时忽略Session这个概念）
简单来说RBAC就是：用户关联角色，角色关联权限。另外，RBAC是可以模拟出DAC和MAC的效果的。
ABAC 基于属性的权限验证
不同于常见的将用户通过某种方式关联到权限的方式，ABAC则是通过动态计算一个或一组属性来是否满足某种条件来进行授权判断（可以编写简单的逻辑）。属性通常来说分为四类：用户属性（如用户年龄），环境属性（如当前时间），操作属性（如读取）和对象属性（如一篇文章，又称资源属性），所以理论上能够实现非常灵活的权限控制，几乎能满足所有类型的需求。

1226
https://magnetw.app/

如上文所述，ThreadLocal 适用于如下两种场景
http://www.jasongj.com/java/threadlocal/
每个线程需要有自己单独的实例
实例需要在多个方法中共享，但不希望被多线程共享

    SourceIp：访问服务的用户的源地址ip
    UserName：访问服务的用户的用户名
    GroupName： 访问服务的用户所属的组
    RoleName： 访问服务的用户所属的角色
    CurrentTime： 访问服务的时间
    Resource： 访问服务的资源名字，例如对于inceptor表，表示方法为default.alice_tbl；对于hdfs目录则为 /user/alice等

局限：目前支持ABAC的插件只有Incpetor的插件，其他服务的插件正在开发中

找出v1中使用abac的部分并对应找出相对v2的部分
已知abac目前只对inceptor有用 对应resource的部分
表 gs_abac_policies gs_policy_statement gs_resource_services

http://172.16.1.41:10080/guardian/guardian/blob/master/docs/ABAC.md

ABAC模型由如下几个部分组成：

PEP(Policy Enforcement Point): 负责使用ABAC策略保护用户数据以及应用。PEP处理访问请求，获得环境上下文，并将这些信息发送给PDP。
PEP的功能是由Guardian Plugin实现的。
PDP(Policy Decision Point): 是ABAC的处理单元，根据所有的策略和PEP发送的请求来决定是Allow还是Deny，PDP使用PIP获得策略和
属性等信息。PDP单元应该是一个可以复用的模块，可以放置到Guardian Server中通过API判断权限；并且需要可以单独提取出来，放到Guardian Plugin
中，在宿主服务中直接判断权限，并借以PIP创建的缓存以及PDP自身的缓存，最大限度的提升权限判断的性能。
PIP(Policy Information Point): 搭建PDP到外部数据源（LDAP数据目录或者关系型数据库）的桥梁。
PAP(Policy Administration Point): 策略管理终端，Guardian Server扮演了PAP的角色。

PEP->PDP->PIP->PAP
 guardianAbacEnabled = this.guardianConf.getBoolean(GuardianVars.GUARDIAN_ABAC_AUTHORIZATION_ENABLED.varname,
            GuardianVars.GUARDIAN_ABAC_AUTHORIZATION_ENABLED.defaultBoolVal);

    if (guardianAbacEnabled) {
      try {
        policyEngine = PolicyEngineFactory.getInstance(this.guardianConf);
        policyEngine.start();
      } catch (GuardianClientException e) {
        LOG.error("Fail to initialize Guardian policy engine when initializing Guardian AccessController", e);
        // TODO: deal with ErrorMsg
        throw new HiveAuthzPluginException(e,
                "Fail to initialize Guardian policy engine when initializing Guardian AccessController", ErrorMsg.GENERIC_ERROR);
      }

v1
PEP: checkPermission->checkPolicy->
PDP: policyEngine.checkPolicy->PolicyUtil.checkStatement->PolicyUtil.checkPolicy
getRelatedStatement Exprs
PolicyUtil.checkStatements(expr.evaluate(cloneContext))->PolicyUtil.evaluate->StatementExpression.evaluate->exp/operator
->guardianClient.getstatements->PolicyController.getStatements->PolicyManager.->policyDao.getStatements->PolicyDaoImpl.getStatements->mapper.getStatements
gs_policy_statement -> statement_id policy_id json
一个策略可以是允许(ALLOW)类型或者拒绝(DENY)类型的策略，允许类型的策略表示，当上下文中的标签满足满足条件的时候允许用户访问资源，而拒绝策略则相反，当上下文中的标签满足条件的时候，则拒绝访问资源。注意，DENY的优先级高于ALLOW，也就是如果一组策略都满足，那么存在任意一个DENY策略，访问都会被拒绝。点击右上角的“拒绝策略”按钮进行切换。默认情况下是ALLOW类型策略，点击之后左上角会出现一个红色的拒绝图标，表示改策略为一个DENY类型策略。再次点击则切换回ALLOW策略。

PIP: PolicyController->PolicyManager->policyDao->PolicyDaoImpl->PolicyMapper
PAP: 终端

v2
PolicyEngine.checkPolicy->getRelatedStatementExprs/PolicyUtils.checkStatements->
StateExpressions.evaluate->Expression.evalate/opertor

gs_statement_resources
gs_resources
gs_abac_policies
gs_policy_statement

etc/conf/guardian加数据库密码


controller.userpermission->findPrincPermissions->permManager.getPrincPerms->permDao.searchPrincPermsOnService->permDaoImpl.searchPrincPermsOnService->permMapper.selectServiceUserPerms->
1. abac guadian版本适配问题 
2. v2权限需要加action过滤 1
3. abac的resource资源变动是否影响
选择所有与此次权限判断的用户、资源、上下文相关的policy, 对策略权限进行判断。ResourceExpr继承于StringExpr，contail模式
获取到json模式转成resourcevo做匹配
每次启动根据请求把对应policy的statement拉下来，并形成关系树。查找和resource相关的
Node used to record a resource node and related policies

192.168.76.128：22

minidwep

1227

findPrincPermissions(SessionVo sessionVo, PrincipalVo principalVo,
                                                  String component, List<String> dataSource,
                                                  String substring, String action, boolean subtree, Boolean inheritance)
selectServiceUserPerms0
selectServiceUserPermsByRole0
selectServiceRolePerms0
selectServiceGroupPerms0
selectServicePermsByGroups0
selectServicePermsByGroupsRoles0


selectUserResourcePerms0
selectRoleResourcePermsByUser0
selectRoleResourcePerms0
selectGroupResourcePerms0
selectGroupsResourcePerms
selectRolesResourcePerms0

selectUserPerms
selectRolePermsByUser
selectGroupsPerms
selectRolesPerms

        <if test="searchValue != null">
            AND MATCH(R.path) AGAINST (#{searchValue})
        </if>

selectGroupAuthorizedResources
selectRoleAuthorizedResources
selectRoleAuthorizedResourcesByUser
selectUserAuthorizedResources
selectGroupsAuthorizedResources
selectRolesAuthorizedResources

getAuthorizedResources

开启abac时
用户执行操作 先检查是否存在关联操作关系的策略，
server会抽取所有与此次权限判断的用户、资源、上下文相关的policy，判断在抽取到策略中是否存在显式的Deny。具体会把所有的statement拉下来，形成一颗关系树。根据操作涉及的resource提取相关的策略exp进行判断是否存在deny。这个策略中存在显式deny的话，拒绝；存在deny_depends并且不存在allow的时候拒绝。存在allow的时候允许；存在not_apply的时候not_apply,默认结果为deny。
用户先拿到操作的resource和action，然后checkpolicy，传的参数是user，resource，action和关于ip和时间的context
server端开启一端缓存，并拉下对应操作组件的所有policy的node，形成一颗关系树
Node used to record a resource node and related policies
找到和要操作的resource相对应的statementExprs
具体做法：1.先找到特定资源的statement，并拉和它有关系的子节点
2.找global statement
3.找孤儿statement
返回之后判断是否通过evaluate
由于每次开启abac都会拉statement判断，不涉及其他字段对应v1和v2的改动
getRelatedStatementExprs

1.fetch statement时会不会有冲突->来源未知 swagger传serviceid 应该不包括字段冲突的情况
2.和resource验证时会不会有冲突

