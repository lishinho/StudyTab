11.4
在目前Guardian的实现中，SHOW TABLES不需要权限，任何用户可以select出所有数据库的所有表；有客户需要对SHOW TABLES进行权限管控，所以需要设计这个操作的权限。
1. diff文件查看
2. git安装
3. 了解guardian 以及对应权限代码

两个组security和guardian代码整体架构 get
工作流程 get

http://172.26.5.93:8180/#/services/12/roles guardian图
diff文件是什么意思 get

11.5
TDH安全-guardian

kerberos
1.ticket
集群中的每一个服务都是一个公园，要进入这些公园（访问服务）你需要各个公园的门票（Service Ticket）。而有了公园联票（Ticket-Granting-Ticket），你就可以不需要单独获取各个公园的门票（Service Ticket），只需要出示联票(Ticket-Granting-Ticket)，便可进入各个公园（访问服务）。
2.principal
在Kerberos认证系统中，principal相当于用户名，是Kerberos给予Ticket-Granting-Ticket的对象。
3.keytab文件
要通过Kerberos的认证需要提供principal及其对应的密码。密码可以手动输入，也可以存放在一个keytab文件中。“Keytab”是“key table”的简写，它用于存放一个或多个principal的密码。
have lessons:micro-serve architecture/distributed file system

11.6
kerberos centrralized managing & login on single point & timeout for serving ->apacheds
Inceptor基于Hadoop技术平台打造，是高效的批处理分析型数据库，可以有效解决大规模数据处理和分析的多种技术难题。社区版Inceptor完整支持SQL 2003标准，提供标准JDBC/ODBC连接支持，从而方便对接第三方软件。
LDAP -> lightweight loging on single point protocol, tree architecture
download sth
get familiar with the product
have lessons:micro-serve architecture/distributed file system
学习git使用，开始读代码
thrift?

11.7
在目前Guardian的实现中，SHOW TABLES不需要权限，任何用户可以select出所有数据库的所有表，有客户需要对SHOW TABLES进行权限管控，所以需要设计这个操作的权限。
->find the operations need priviledges and set the priviledge for the show tables operation.
download all things and begin to read code
1.装好maven，jdk，输入法。intelliJ快捷键查找，学习maven的使用
2.读代码，找路径

11.8
学会安装自动化测试集群，并开始一次简单的测试。
hive项目的版本号？
<parent>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive</artifactId>
    <version>8.0.1</version>
    <relativePath>../pom.xml</relativePath>
  </parent>
<dependency>
      <groupId>org.apache.hive</groupId>
      <artifactId>inceptor-metastore</artifactId>
      <version>${metastore.version}</version>
    </dependency>
Docker安装集群

root/Transwarp!
tw-node1236 172.16.1.236
tw-node1237 172.16.1.237
tw-node1238 172.16.1.238

docker:镜像+容器+仓库 容器虚拟化
拉取新镜像docker run --name [容器名] -p [主机端]:[映射端口] [仓库名]
根据官方wiki解压包
https://www.warpcloud.cn/#/documents-support/docs-detail/document/TDH-OPS/6.2/010InstallManual?docType=products&docName=TDH%E5%AE%89%E8%A3%85%E6%89%8B%E5%86%8C

curl -X GET -u wenxiang.qiu:123456 "http://172.16.1.97:8080/remote.php/webdav/OFFICIAL/MANAGER/manager-6.0.1910a-final/IMAGE/centos-7/MANAGER-Basic-Component-Transwarp-6.0.1910a-final.tar.gz" > /var/lib/docker/manager.tar.gz

curl -X GET -u wenxiang.qiu:123456 "http://172.16.1.97:8080/remote.php/webdav/OFFICIAL/TDH/transwarp-6.2.0-final/IMAGE/centos-7/TDH-Image-Registry-Transwarp-6.2.0-final.tar.gz" > /var/lib/docker/tdh.tar.gz

下载tdc和manager
source /etc/profile打开maven

第一周总结
熟悉产品TDH和guardian，安装集群测试，一些基础概念协议和工具的使用，比如git，docker，kerberos，LDAP协议
针对问题，在之前的工作总结出两种方法。一种是基于guardian插件修改上层代码，设立showtable的开关和权限；另一种是在hive工程的metastore中用metastorehook来控制。这两种都需要改动hive的代码和接口，难度不难但有一定的改动量需要时间去看。第一种方法改动上层接口，偏向于权限控制，第二种在hive里改会更彻底但会造成性能问题。
针对后一种提出改动方案是夹一层中间缓存，但要控制缓存不一致问题。
现在提出的具体想法是先按照完成第一种方案，拿到集群上看效果，并测试到成熟的版本


第二周
11.11
上午把集群测试完毕
下午集中解决问题 并再完成一版测试

1. 任务完成，docker和kuber命令，使用命令行把本地的jar包放到虚拟机上再用docker命令打到镜像中，然后替换jar包，重启inceptor测试test的无权限show表。
明天把类的关系梳理清楚，DDL类需要再看一下，接口的概念再好好看，工程流程找资料熟练一些。把改动争取做出来。测试指令走两轮。

11.12
节点崩了怎么办-》237节点重启，控制权不在本地，docker不可用原因是没有配置预先挂载好的文件将查到的UUID值<UUID>添加在/etc/fstab中：confluence	#1
gardian本身没有接收控制权限的文件，在本地修改后的代码要在hive端使用接口才能生效
几个查找方式指令k8s和docker
docker常用指令：https://blog.gtwang.org/linux/docker-commands-and-container-management-tutorial/
mvn常用指令：https://blog.csdn.net/zhglance/article/details/54945104
k8s常用指令：https://blog.csdn.net/xingwangc2014/article/details/51204224

视频处理项目实例
ISP通用技术 输入-解码-前处理-推理-后处理-编码-输出
1.输入：V4L2 摄像头-后端连接ONVIF，V4L2 有专设的接口
2.编码：利用时间冗余，空间冗余，感知冗余 用更少的字节数存储更多的视频量 H.26x VPx AVS
3.硬件加速/场景加速 用在编码解码-算法部分
4.推理：GPU编程模型 主存（输出）<->显存（中介）<->GPU（计算/推理）
两种方案：推理服务器/本地推理（SDK）
5.FAAS 数据结构转换/模型服务代理（推理代理）
渲染：（推理-推流（边缘端））-（流媒体服务（边缘端/云端））-（拉流-解码-渲染（浏览器））

AAA
身份认证/权限管理/审计
1. 安全隐私
2. 
3.
4. RBAC基于角色，ABAC基于审计的权限管控模型

11.13
237节点的 inceptor-server崩溃无法进入认证模式 --可以在TDC端设置产品依赖，减小节点压力，在服务端重启服务

jvm的gc总是limit overhead，调整jvm最大可用堆和初始堆大小仍然没用
用Maven是出现OutOfMemory错误，即使设置了MAVEN_OPTS也不起作用
在/etc/profile中添加MAVEN_OPTS参数

指令进入ssh之后。docker images kubectl get po | grep inceptor
mvn install -DskipTests
export MAVEN_OPTS="-Xmx8192m -XX:MaxPermSize=2048m"
beeline -u "jdbc:hive2://localhost:10000/default;principal=hive/tw-node1237@TDH"

beeline简单指令
create table test.testTb(id int) tblproperties('author'='lishinho');
show tables in test
create database db2
select * from test.testtb

ctrl+shift+T在终端窗口新开一个终端
ls -al /root显示操作日期

  private static List<String> mergeSortedTableLists(List<String> tblsWithPrivs, List<String> tbls) {
//    List<String> mergedResult = new ArrayList<>();
//    int i1 = 0, i2 = 0;
//    while (i1 < tblsWithPrivs.size() && i2 < tbls.size()) {
//      int compare = tblsWithPrivs.get(i1).compareTo(tbls.get(i2));
//      if (compare < 0) {
//        i1++;
//      } else if (compare > 0) {
//        i2++;
//      } else {
//        mergedResult.add(tbls.get(i2));
//        i1++;
//        i2++;
//      }
//    }
//    return mergedResult;
    tbls.retainAll(tblsWithPrivs);
    return tbls;
  }
明天解决所有bug提交第一版


11.14

代理模式项目有自带的代理写在配置文件
代码不生效
测了3次都失败 怀疑是代码问题？
klist

la -al /root
1.改动guardian端代码优化，hive代码优化上移-无效
2.略去globalpriv方法的代码和show开关代码-无效
3.tbls.clear()-无效
结论改得代码好像没有起作用
可能缘由：1. 测试过程有问题，代码没有提交到镜像运行
2. 代码配置文件没有调用到guardian端代码？
3. guardian代码修改被避开了，因为测试的权限用户都有

原因：每次push镜像之后要重启inceptor才能生效，查询日志 vim /var/log/inceptor1/hive-server2.log\
s设置断点

测filtertable是否好用。改hasanytable方法

634-657
 boolean hasAnyTablePrivilegesOnDB(String username, String dbName) {//do sth
    try {
      boolean isDBOwner = SQLAuthorizationUtils.isOwner(username, Collections.EMPTY_LIST,
              new HivePrivilegeObject(HivePrivilegeObject.HivePrivilegeObjectType.DATABASE, dbName, null));
      if (isDBOwner) {
        return true;
      }
    } catch (HiveAuthzPluginException ex) {
      LOG.error("Failed to check if user [{}] is the owner of database [{}].", username, dbName, ex);
      // go on
    }
    List<String> dataSource = InceptorPermUtil.convertDatabase(dbName);
    List<PermissionVo> permVos = new ArrayList<>();
    permVos.add(new PermissionVo(component, dataSource, ADMIN_PERM));
    for (GuardianSQLPrivilegeType privilegeType : GuardianSQLPrivilegeType.ALL_ON_TABLE) {
      permVos.add(new PermissionVo(component, dataSource, privilegeType.name()));
    }
    try {
      return guardianClient.checkAnyAccess(username, permVos);
    } catch (GuardianClientException e) {
      LOG.error("Fail to check if user [{}] has any table privileges on database [{}]", username, dbName, e);
      return true;
    }
  }

测试一结果：
show databases时 赋表权限没有上传的database上
show tables不可用，全为空
分析：show databases时原生写的函数只检查赋予db上的权限，并没有审查赋予db中表的权限，权限没有上移->把相应代码修改成guardian拉取全部权限判断是否为空
show tables没有调用到实体，只调到引用，简单修改一下
晚上能跑两次？

bash运行：
开两个终端
1. ssh root@172.16.1.237
ll
2.
scp '/home/transwarp/Downloads/guardian-plugins/plugins/inceptor-plugin/target/inceptor-plugin-transwarp-6.2.1.jar' root@172.16.1.237:/root
scp '/home/transwarp/Downloads/work/hive-0.12.0-transwarp/src/ql/target/inceptor-exec-8.0.1.jar' root@172.16.1.237:/root
1. ll
// docker ps | grep inceptor
docker images | grep inceptor
docker ps | grep 968
docker cp inceptor-exec-8.0.1.jar xxx:/root
docker cp inceptor-plugin-transwarp-6.2.1.jar xxx:/root
2. docker run -it 968 bash
ls

ls
cd /usr/lib/inceptor/lib
ls
[root@50af60a9df09 lib]# mv inceptor-exec-8.0.0.jar /root
[root@50af60a9df09 lib]# mv inceptor-plugin-transwarp-6.2.0.jar /root
[root@50af60a9df09 lib]# ls /root
anaconda-ks.cfg          inceptor-exec-8.0.1.jar              inceptor-plugin-transwarp-6.2.1.jar
inceptor-exec-8.0.0.jar  inceptor-plugin-transwarp-6.2.0.jar  R-3.3.1-21.x86_64.rpm
[root@50af60a9df09 lib]# mv /root/inceptor-plugin-transwarp-6.2.1.jar ./
[root@50af60a9df09 lib]# mv /root/inceptor-exec-8.0.1.jar ./
ls
1.
docker commit 8dc tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final

docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final



[root@tw-node1237 ~]# docker commit 52f tw-node1236:5000/transwarp/guardian:guardian-3.1.0-final 
重启inceptor
kubectl get po -owide | grep inceptor
kubectl exec -it inceptor-server-inceptor1-697b7f4665-ctfbq bash
klist
kinit
beeline -u "jdbc:hive2://localhost:10000/default;principal=hive/tw-node1237@TDH"

测试2 show databases没有过滤；show tables admin角色没有显示 test完成
filter加上了global权限的审核

admin代码全部能跑通，test的show databases没有显示表，日志显示全部被过滤掉了


主要的时间在学习docker和k8s的使用处理上，有一个逐渐熟悉的过程，1 hive端的ddltask里面的两个指令中增加逻辑调用接口，2 之前这个项目的代码也优化了权限过滤的逻辑，结果 show tb和db都可以完成权限过滤，
存在问题：filter开关文件指向有一个问题，找到识别的路口，封装。
潜在问题 1.性能上影响 2.权限设置上 根据需求看还需不需要改动一下逻辑
pre-commiter
merge request?

修改xml文件 在etc里

https://172.16.1.237:8380/swagger-ui.html#!/login/loginUsingPOST

1118
总结
修改上次的代码并跑通逻辑
hive的metastore修改代码为什么会有性能问题
precommit->wiki搜inceptor precommit
放空

1119
熟悉git操作，发送merge request
提交
git rebase --abort //执行rebase的丢弃工作
git fetch trunk 8.0 //trunk是分支 8.0是分支号？ 執行 fetch，可以取得遠端數據庫的最新歷史記錄。取得的提交會導入在自動建立的分支中
git rebase trunk/8.0
git log
git push origin -f
rebase 分支前，先执行 fetch 下载最新的代码，否则可能会存在最终不能 push 的情况。
尽量少用 --force 或者 -f 参数，进行强制性操作。、
git branch -D Warp-31985
git branch -m Warp-31985
git push origin -f
git cherry-pick 51089e712f449abdb621b81a0d8b44b16a5fc856
git commit -am "WARP-31985:Add Show databases/show tables permission check"
git pull origin master --rebase

spring boot
解决metastore端的代码，熟悉guardian问题
知道类名查找类:Ctrl+Shift+Alt+N; 
怎么远程调试设置断点

spring Boot
简化配置，易于打包，第三方集成
spring加载器loader封装
spring的eventlistener可以用
AOP&IOC

1120
浏览器开发者模式ctrl+shift+I
git fetch origin WARP-31985
rename
git checkout WARP-31985

两个独立分支的代码合并有两个比较常用的命令，分别是 git cherry-pick <commit-id>和git rebase。cherry-pick  主要用于copy某个单独分支的patch，而rebase主要用于一次性合并整个分支。以下主要介绍rebase：

语法：git rebase upstream-branch to-branch  （要变基的分支是后者，如果当前分支为to-branch，可以省略后者分支）
git checkout -b "分支名" 创建并切换分支

tcc是一个TDC的租户管理中心
TCC删除产品时需要以tcc用户把该产品中注册到guardian的服务删除, 为了减少网络开销, 需要guardian提供批量删除服务的接口

https://172.16.1.51/tdc/guardian/swagger-ui.html#/      swagger api
user resources
https://172.16.1.51/tdc/guardian/#/home?dataSourcePrefix=PATH     guardian

package io.transwarp.guardian.server.boot.controller/resourceServiceController    @DeleteMapping("/services/{serviceType}/{serviceName}") 104line

package io.transwarp.guardian.client/guardianAdmin/removeService()    234
package io.transwarp.guardian.client/guardianClient

idea快捷键
ctrl+shift+Alt+N
ctrl+F

注释
@PathVariable     --》请求url
spring mvc中的@PathVariable是用来获得请求url中的动态参数的

@RequestParm    --》请求参数

@Audit注解的类或方法在执行前会先进这个方法，记录调用日志。 

@ApiOperation不是spring自带的注解是swagger里的
@ApiOperation(value = “接口说明”, httpMethod = “接口请求方式”, response = “接口返回参数类型”, notes = “接口发布说明”；其他参数可参考源码；

@ApiParam(required = “是否必须参数”, name = “参数名称”, value = “参数具体描述”
Swagger是当前最好用的Restful API文档生成的开源项目，通过swagger-spring项目
实现了与SpingMVC框架的无缝集成功能，方便生成spring restful风格的接口文档，

@GetMapping、@PostMapping、@PutMapping、@DeleteMapping、@PatchMapping
 @GetMapping是一个组合注解，是@RequestMapping(method = RequestMethod.GET)的缩写。该注解将HTTP Get 映射到特定的处理方法上。


GuardianVars识别不到->和master工程show diff一下并改正
Guardian & ApacheDS 本地编译及启动方法  http://172.16.1.168:8090/pages/viewpage.action?pageId=22692984
fork/clone guardian项目，jacocoagent，mysql，protobuf 通过swagger控制api

11.21\
在日志guardianlog里看结果

1. 测试流程 退出swagger
2. 无法建立索引连接
3. 需求；removeall
如果servicetype和servicename是一对多的关系需要一个对应getservicetype所以servicename的接口-serviceManager
如果是多选servicename，那么需要传递一个list<ServiceVo>的接口在remove里面改-前端怎么传list

//传list到前端api
  @DeleteMapping("/services")
  @ApiOperation(value = "Delete List services", notes = "login is needed")
  @Auditable(field = AuditField.SERVICE, requestClass = "RemoveServiceRequest", level = AuditLevel.DELETE,
          operationFormat = "remove services: [%s]")
  public void removeServices(HttpServletRequest request,
                             @RequestParam("serviceName") List<String> serviceName) throws GuardianException{
    Session session = (Session) request.getSession().getAttribute(Constants.FORTRESS_SESSION);
    resourceServiceManager.removeList(session, null, serviceName);
  }

  @Override
  public void removeServices(final List<ServiceVo> ServiceVos) throws GuardianClientException {
    runWithRelogin(new RestWork<Void>() {
      @Override
      public Void run() throws GuardianClientException {
        for(ServiceVo serviceVo : ServiceVos){
          restClient.delete(RESOURCE_SERVICES + "/" + encode(serviceVo.getServiceType())
                  + "/" + encode(serviceVo.getServiceName()));
        }
        return null;
      }
    });
  }

guardian/test目录下执行bash startGuardianServer.sh
日志文件中查看结果
guardian swagger的使用，先到user处login（admin-123）然后resource-service-register添加ServiceVO，services中查看，deleteapi中测试
localhost:8380


主要分为5个子项目：

    apacheds-backend: apacheds的主程序以及启动初始化的schema
    apacheds-parent: apacheds的核心实现
    directory-fortress-core: fortress是基于LDAP的RBAC模型的一种实现
    guardian: guardian server后端代码
    lmdbjni: lmdb为apacheds底层使用的数据库

shift+printscreeen截图

1122
完成测试部分代码，并跑通
下午开会
完成重构

本地测试跑不通->项目的配置文件是否配制成正确的访问地址和端口
线上测试跑不同->在项目中添加角色sobar帮助项目CI审核
remove测试写法->用assert判断，加日志，
先检测是否添加service成功Assert，操作，检测是否操作成功Assert，跑一遍
还剩的问题dao层的deleteservices无法访问到接口，client层的不会配置文件？无法访问接口
swagger上插入了test代码，说明test成功只是没有client去访问，每次测试开了一个新的session刷新server数据
为什么要改dao层？？
db-dao-sevice-controller-view层


<delete id="deleteParamsByIds">

        delete from ts_sys_params where id in

        <foreach collection="array" open="(" close=")" item="item" separator="," >
            #{item}
        </foreach>
</delete>


    <delete id="deleteServices" parameterType="java.util.List">
        DELETE FROM gs_service WHERE service_name in
        <foreach collection="serviceNames" open="(" close=")" item="item" separator="," >
            #{serviceNames}
        </foreach>
    </delete>
sql在xml里写删除遍历list
后台控制构造URL的方法，qs string，？
学会找返回值
配置文件


1. WARP-31985：修改在g-plugin，hive后端代码 完成添加guardian端和hive端的show tables和show databses的接口
跑过了precommit，需求逻辑在我的集群跑过了，1. 里面的guardianvars有字段标红应该是ide的问题，然后我在提交的代码上是直接标称true了，
2. WARP-39221：完成添加批量删除注册服务的接口
修改位置guardian后端，完成功能-test目录里的test测试-看日志/代码规范/版本管理/用swagger UI，guardian和apacheds的本地编译
3.  熟悉git命令和项目测试方法


git rebase trunk/master
git reset HEAD~1
git reflog

本周：
1. WARP-39221：完成项目关于改动的重构部分
2. WARP-31985: 
2. guardian端代码的继续熟悉学习

11.25
添加批量删除注册服务的接口

CI 是持续集成。CD 是持续部署。
SPNEGO是一种使用GSS-API(通用安全服务应用接口)认证机制的安全协议。 

guardian/guardian-client/src/test/java/io/transwarp/guardian/client/impl/RemoveServiceListTest.java
不要提交不用的文件
return后加分号不需要空格
CollectionUtils.isEmpty(serviceNames)可以替代== null || .size()==0
不要没用地调多次系统函数
注意一下非static final变量名，以及方法名都用驼峰命名法
一般是SQL里面的保留字或者关键字都大写
一般单个变量名叫serviceName的话，集合形式的就叫serviceNames
花括号前面要加个空格
guardian 的项目都是空两个空格的，所有的提交都检查一下
心跳问题
    GuardianConfiguration guardianConfiguration = new GuardianConfiguration();
    guardianConfiguration.setLong(GuardianVars.GUARDIAN_CLIENT_HEARTBEAT_REPORT_INTERVAL.varname,
        500L);


Etcd是CoreOS开源的一个高可用强一致性的分布式存储服务
Kubernetes使用Etcd作为数据存储后端,把需要记录的pod、rc、service等资源信息存储在Etcd中
Etcd使用raft算法将一组主机组成集群,raft 集群中的每个节点都可以根据集群运行的情况在三种状态间切换:
follower、candidate与leader。
leader 和follower 之间保持心跳,如果follower在一段时间内没有收到来自leader的心跳,就会转为candidate,发出新的选主请求。当一个节点获得了大于一半节点的投票后会转为leader节点

API Server提供了k8s各类资源对象(pod,RC,Service等)的增删改查及watch等HTTP Rest接口,是整个系统
的数据总线和数据中心。
在 kubernetes 集群中,API Server 有着非常重要的角色。API Server负责和etcd交互(其他组件不会直接操作
etcd,只有 API Server 这么做),是整个 kubernetes 集群的数据中心,所有的交互都是以API Server为核心
的。简单来说,API Server 提供了以下功能:

整个集群管理的API接口:所有对集群进行的查询和管理都要通过API Server来进行
集群内部各个模块之间通信的枢纽:所有模块之之间并不会之间互相调用,而是通过和 API Server 打交道来完成自己
那部分的工作
集群安全控制:API Server 提供的验证和授权保证了整个集群的安全

Controller Manager 是一个集群内部的管理控制中心,有一组控制器构成,这组控制器负责集群内部的 Node、
Pod、Endpoint、Namespace、ServiceAccount、ResourceQuota 等等资源的管理。
• 每个Controller通过API Server提供的接口实时监控整个集群的每个资源对象的当前状态,当发生各种故障导致
系统状态发生变化时,会尝试将系统状态修 复到“期望状态”

负责集群的资源调度,为新建的pod分配机器
根据特定的调度算法将pod调度到指定的工作节点(Node)上,这一过程也叫绑定(bind)。Scheduler的输
入为需要调度的Pod 和可以被调度的节点(Node)的信息,输出为调度算法选择的Node,并将该pod bind到该
Node

调度过程分为两步, predicate以及prioritize
predicate筛选满足条件的node
prioritize给剩下node打分,选择分数最高的node,作为bind的node

kube-proxy负责service的实现,即实现了k8s内部从pod到service和外部从node port到service的访问。

集群中的每个 Node 都有 Kubelet 进程,该进程用于处理 Master 节点下发到本节点的任务,管理 Pod 以及
Pod 中的容器。
节点管理:kubelet 启动时向 API Server 注册节点信息,并定时向 API Server 汇报节点状况;
Pod管理:创建/删除 Pod,下载容器镜像,用 Pause 创建容器,运行容器,校验容器是否正确等;

kubelet
容器健康检查:通过访问容器的 HTTP 接口(HTTP 状态码作为判断依据)来判断容器是否健康;
cAdvisor 资源监控:cAdvisor 集成到 kubelet 程序的代码之中,负责查找当前节点的容器,自动采集容器级别的 CPU、内存、文件系统和网络使用的统计信息。
////

通过将系统内部的对象“分配”到不同的Namespace中,形成逻辑上分组的不同项目、小组或用户组,便于不同
的分组在共享使用整个集群的资源的同时还能被分别管理。
与Resource Quota(配额)一起提供多租户管理
Kubernetes集群在启动后,会创建一个名为“default”的Namespace,如果不特别指明Namespace,则用户创建的Pod、RC、Service都被系统创建到“default”的Namespace中
kube-system是预留的命名空间,系统服务在kube-system下运行
可以通过配置RBAC, 每个用户只能使用自己命名空间下的资源


用于限制命名空间下资源的使用
命名空间下如果没有配额, 则视为无限制
可以限制cpu,memory,存储卷等
目前主要是限制pod的创建

Resource Quota (配额)

pod无法在带quota的命名空间下创建
超出配额
pod中的容器没有资源使用声明


K8S中最小的调度单位
一个pod只会调度到一台机器,不会横跨两台机器
一个pod调度完成之后,不会移动到其他机器
容器组中包含一个或多个容器
•
•
Pod (容器组)
• 共享网络空间
• infra 容器用于维持容器组IP
• 共享存储卷
•
来源:
• 用户可以创建pod
• Job,Rs,Deployment,Statefulset产生

Pod生命周期
Pending (pod中至少还有一个容器还没有启动)
Running (pod中所有容器都启动了,并且至少一个容器还在运行中)
Succeed (pod中所有容器都退出了,并且都成功退出)
Failed (pod中所有容器都退出了,并且至少一个容器失败退出)
Unknow (无法或者容器状态)

11.26
抽象类，list初始化

git rebase -i HEAD~21 合并多个commit到一个（要观察head的位置）
把文件中的操作换为s，ctrl-X yes然后保存，然后关闭
    <delete id="deleteServices" parameterType="java.util.List">
        DELETE FROM gs_service WHERE service_name IN
        <foreach collection="serviceNames" open="(" close=")" item="item" separator="," >
            #{item}
        </foreach>
    </delete>
注意item sql写在xml里的方法

Guardian-V2重构
ctrl点击项目名 快捷查找

  @Override
  public void removeServices(final List<ServiceVo> ServiceVos) throws GuardianClientException {
    runWithRelogin(new RestWork<Void>() {
      @Override
      public Void run() throws GuardianClientException {
        final QueryString qs = new QueryString();
        for (ServiceVo serviceVo : ServiceVos) {
          qs.add("serviceName", serviceVo.getServiceName());
        }
        httpClient.delete(RESOURCE_SERVICES + qs);
        return null;
      }
    });
  }

git rebase trunk/master
git rebase -i HEAD~1
git reflog

完成WARP-31228删除服务list的逻辑

1127
linux tab命令自动补全
重构项目/目的-1.apacheds-backend里面没有东西->统一融合到guardian后端代码 2.一大堆mf文件->apacheds相关，不用管
报错 没改的地方-》测试文件用到扥都是同一数据库，所以会互相影响数据，在dao层的操作记得sessioncommit
guardian跑一次太慢0了8

1.session没有commit
2.方法错了，不是deleteserviceperms
3.路径

  @Override
  public List<UserVo> getGroupOwners(final String groupName, final boolean inheritance) throws GuardianClientException {
    return runWithRelogin(new RestWork<List<UserVo>>() {
      @Override
      public List<UserVo> run() throws GuardianClientException {
        QueryString qs = new QueryString();
        qs.add("inheritance", inheritance);
        return httpClient.get(GROUP_URL + "/" + encode(groupName) + "/owners" + qs.toString(), new TypeReference<List<UserVo>>() {
        });
      }
    });
  }



DAO（Data Access Object）顾名思义就是用 OO 的方式去执行数据库的操作，包括函数化的 CRUD，可以自己去写也可以用 ORM 框架。Service 可以处理事务和业务逻辑，比如用户登录的校验等。Controller 其实就与业务无关了，它更多的是将 Service 层的结果加以处理返回给 View，也可能会处理一些简单的参数检验工作。Util 的话就是一些辅助类（或函数），涉及面可能比较广，粒度小，比如生成 MD5 的便利函数等。Model 是最简单的，Java 里对应的就是 Beans，ORM 框架也将根据 Model 中的定义去生成 SQL 语句，在其他语言中也是一些 Plain Old Object，自身可能会有一些数据表达的处理功能，但更多的不应该涉及副作用。

view-controller-service-dao-db
Controller-->service接口-->serviceImpl-->dao接口-->daoImpl-->mapper-->db
util,model

在SpringMvc后台进行获取数据，一般是两种。 
1.request.getParameter(“参数名”) 
2.用@RequestParam注解获取

@RequestParam 参数标识注解存在三个参数
1、value值
对应url提交的?id=xx中的id，参数名

2、defaultValue 该参数的默认值
当url中id值没有传输时将使用这个参数进行赋值

3、required
是否必须。默认为 true, 表示请求参数中必须包含对应
的参数，若不存在，将抛出异常

完成WARP-31228的重构

11.28
两个blog：
1. dao-service-controller层 1
2. git的利用
3.业务，群，部门，岗位 1
4. spring常用注解
组件类注解
@component->spring bean
@repository->dao组件
@service->业务逻辑组件
@controller->控制器组件

11


Web 层:主要是对访问控制进行转发，各类基本参数校验，或者不复用的业务简单处理等。
git rebase trunk refactor/WARP39221
trunk是本地的一个分支，按照gitlab开发流程来
V1和V2的removeService还是像原来一样分开写吧，否则@PathVariable("serviceType") String serviceType  swagger显示是必填的，但是V2又没用到
代码规范 大括号，for循环，空格
tdh是产品hub，tdc是管理云
tcc是tdc的租户管理中心组件

http://172.16.1.168:8090/pages/viewpage.action?pageId=22692984  warp39221开发流程

warp-39528
创建/删除互信API的target domain admin username & password不再是必须参数
需要工具：https://172.26.5.98:8380/swagger-ui.html#!/trustRelations/addTrustRelationUsingPOST
https://172.26.5.98:8380/#/setting/trustRelation
修改范围：master/guardian-backend/trustrelationcontroller-trustRelationshipVo-DomainVo-guardianServerVo

accesstoken coffeebabe git rebase trunk/master accessToken???
1.accesstoken是否能替代用户名密码，需要加限制条件吗-查accesstoken的缓存
2.在数据库中是怎么存储domainVo的-普通存
3.怎么验证？只改了domainVo的构造函数？


启动apacheds的坑：java.lang.NoClassDefFoundError:-> 执行mvn clean install -DskipTests -Pcopy-deps
lock：重启bash

docker的使用

1129
CollectionUtils.isEmpty(serviceNames) 
token-AbstractguardianClient

1. dao-service-controller层 
db-dao-service-controller-view
util,entity
大项目会把项目分解成很多不不同的模块（Module），然后根据用途和角色，我们对这些模块有一个通用的命名规则，这也就是上面这些英文单词的来历。数据从前端

dao= data access object 数据存取对象
自动产生SQL语句来和数据库打交道，让我们对数据库的操作看起来比较像和一个对象打交道。这个对象通常就是DAO。
dao对象一般调用mapper来映射sql语句，执行数据库操作，例如
<delete id="deleteServices" parameterType="java.util.List">
        DELETE FROM gs_service WHERE service_name in
        <foreach collection="serviceNames" open="(" close=")" item="item" separator="," >
            #{serviceNames}
        </foreach>
    </delete>

Service，我们有时候会需要一些相对独立，与业务系统没什么关系的功能。但不是所有的功能都可以做成一个服务，服务是一个相对独立的功能模块，完成一些指定的工作，这些工作高度抽象和通用。一个典型的服务像是数据库服务、缓存服务、文件存储服务、身份验证服务、消息队列服务等。
Util，Util通常来说是我们找不到合适的名字的时候的选择，Util就是工具，在做项目的时候我们总会遇到一些奇奇怪怪的小功能或者重复的代码需要提取。像是URL编码或者解码，或是自创的加密签名算法等等。

entity实体，所有系统中出现需要封装成对象映射于现实世界的物体或事件的，我们都可以用实体类封装，或者划分的更细可以是各个object，view视图，看到的前端界面

分层领域模型规约：

DO（ Data Object）：与数据库表结构一一对应，通过DAO层向上传输数据源对象。
DTO（ Data Transfer Object）：数据传输对象，Service或Manager向外传输的对象。
BO（ Business Object）：业务对象。 由Service层输出的封装业务逻辑的对象。
AO（ Application Object）：应用对象。 在Web层与Service层之间抽象的复用对象模型，极为贴近展示层，复用度不高。
VO（ View Object）：显示层对象，通常是Web向模板渲染引擎层传输的对象。
POJO（ Plain Ordinary Java Object）：在本手册中， POJO专指只有setter/getter/toString的简单类，包括DO/DTO/BO/VO等。
Query：数据查询对象，各层接收上层的查询请求。 注意超过2个参数的查询封装，禁止使用Map类来传输。
领域模型命名规约：

数据对象：xxxDO，xxx即为数据表名。
数据传输对象：xxxDTO，xxx为业务领域相关的名称。
展示对象：xxxVO，xxx一般为网页名称。
POJO是DO/DTO/BO/VO的统称，禁止命名成xxxPOJO


1. WARP-39221：完成项目关于改动的重构部分
2. guardian端代码的继续熟悉学习
完成WARP-31228删除服务list的逻辑
warp-39528：创建/删除互信API的target domain admin username & password不再是必须参数
1. 只有token/？
2. 集群


完成：
1. WARP-39221:  完成删除服务list的逻辑并提交代码
2. WARP-39221：完成项目关于改动的V2重构部分
 1-guardian项目 从controller层 service层 改到dao层
 2-git 和代码规范
进行中：
WARP-39528：创建/删除互信API的target domain admin username & password不再是必须参数 逻辑编写/测试

1202
gitlab的compare是和origin对比
WARP-31985
1.跟踪识别不了GuardianVars的内容
2.arraylist的remove方法可能导致逻辑错乱？
list的remove(int index)方法部分
->用list的remove 博文没有改变索引值，导致漏删挨在一起的元素->改变方法是倒序遍历list（动态变化当前下标值也行？）
->不推荐用foreach方法遍历使用remove方法，原因是foreach根据list对象创建一个Iterator对象，用这个迭代对象去遍历列表，而iterator的next方法和list的remove方法一起用会造成ConcurrentModificationException/边界问题->推荐用Iterator中的remove方法去代替ArrayList中的remove方法
->自定义类使用 要注意重写equals方法


public void remove4(ArrayList<Integer> list) 
{
    Integer in = 1;
    Iterator<Integer> it = list.iterator();
    while (it.hasNext()) 
    {
        Integer s = it.next();
        if (s.equals(in)) 
        {
            it.remove();
        }
    }
}


-3369
    //filter databases in ShowDatabasesDesc
    if (authorizer.shouldFilterShowTables() && !authorizer.hasAnyGlobalTablePrivileges()) {
      for (int i = 0; i < databases.size(); i++) {
        if (!authorizer.hasAnyTablePrivilegesOnDB(databases.get(i))) {
          LOG.info("DBdeleteName : " + databases.get(i));
          databases.remove(i--);
        }
      }
    }

拿了39528的集群，开始测试
62003->自己写的不对，conf的set方法写成get

 "org.apache.directory.api.ldap.model.exception.LdapEntryAlreadyExistsException: ENTRY_ALREADY_EXISTS: failed for MessageType :-> 重启



1203
为什么要new一个新的ArrayList并且返回？
在方法被调用时，实参通过形参把它的内容副本传入方法内部，此时形参接收到的内容是实参值的一个拷贝，因此在方法内对形参的任何操作，都仅仅是对这个副本的操作，不影响原始值的内容。
无论是基本类型和是引用类型，在实参传入形参时，都是值传递，也就是说传递的都是一个副本，而不是内容本身。
https://juejin.im/post/5bce68226fb9a05ce46a0476

这里是否考虑到owner的情况？TDH 5.2.4版本之后table创建时不会默认为owner在Guardian中添加权限记录？
这里仍然需要发送num(databases)次rest请求给Guardian Server，在database数量较多的情况下，效率比较低，是否能改成只发送一个请求给Guardian Server获取到用户有权限的database列表，然后与这里的databases取交集？

3367-3376
    //filter databases in ShowDatabasesDesc
    if (authorizer.shouldFilterShowTables() && !authorizer.hasAnyGlobalTablePrivileges()) {
      Iterator<String> it = databases.iterator();
      while (it.hasNext()) {
        String database = it.next();
        if (!authorizer.hasAnyTablePrivilegesOnDB(database)) {
          it.remove();
        }
      }
    }

3438-3444
    //filter tables in ShowTablesDesc
    if(authorizer.shouldFilterShowTables() && !authorizer.hasAnyGlobalTablePrivileges()){
      List<String> tmp = authorizer.filterTablesByPrivileges(dbName, tbls);
      tbls.clear();
      tbls.addAll(tmp);
      tmp = null;
    }



626-722
 boolean shouldFilterShowTables() {
    try {
      return guardianConf.getBoolean(GuardianVars.GUARDIAN_INCEPTOR_FILTER_SHOWTABLES.varname, GuardianVars.GUARDIAN_INCEPTOR_FILTER_SHOWTABLES.defaultBoolVal);
    } catch (ExceptionInInitializerError ex){
      return true;
    }
  }

  boolean hasAnyTablePrivilegesOnDB(String username, String dbName) {
    try {
      boolean isDBOwner = SQLAuthorizationUtils.isOwner(username, Collections.EMPTY_LIST,
              new HivePrivilegeObject(HivePrivilegeObject.HivePrivilegeObjectType.DATABASE, dbName, null));
      if (isDBOwner) {
        return true;
      }
    } catch (HiveAuthzPluginException ex) {
      LOG.error("Fail to check if user [{}] is the owner of database [{}].", username, dbName, ex);
    }
    try {
      List<PermissionVo> permVos = guardianClient.userPermissions(username, component, InceptorPermUtil.convertDatabase(dbName), true);
      return permVos != null && !permVos.isEmpty();
    } catch (GuardianClientException e){
      LOG.error("Fail to filter tables by privileges. username: [{}], dbName:[{}]", username, dbName, e);
      return false;
    }
  }

  boolean hasAnyGlobalTablePrivileges(String username) {
    List<String> dataSource = InceptorPermUtil.global();
    List<PermissionVo> permVos = new ArrayList<>();
    permVos.add(new PermissionVo(component, dataSource, ADMIN_PERM));
    for (GuardianSQLPrivilegeType privilegeType : GuardianSQLPrivilegeType.ALL_ON_TABLE) {
      permVos.add(new PermissionVo(component, dataSource, privilegeType.name()));
    }
    try {
      return guardianClient.checkAnyAccess(username, permVos);
    } catch (GuardianClientException e) {
      LOG.error("Fail to check if user [{}] has any global table privileges.", username, e);
      return true;
    }
  }

  List<String> filterTablesByPrivileges(String username, String dbName, List<String> tbls) {
    try {
      List<PermissionVo> permVos = guardianClient.userPermissions(username, component, InceptorPermUtil.convertDatabase(dbName), true);
      Set<String> tblsWithPrivsSet = new HashSet<>();
      if (permVos != null) {
        for (PermissionVo permVo : permVos) {
          if (!isValidTablePerm(dbName, permVo)) {
            continue;
          }
          String tableName = permVo.getDataSource().get(2).toLowerCase();
          tblsWithPrivsSet.add(tableName);
        }
      }
      List<String> tblsWithPrivs = new ArrayList<>(tblsWithPrivsSet);
      tbls.retainAll(tblsWithPrivs);
      return new ArrayList<>(tbls);
    } catch (GuardianClientException e) {
      LOG.error("Fail to filter tables by privileges. username: [{}], dbName:[{}]", username, dbName, e);
      return tbls;
    }
  }

  private static boolean isValidTablePerm(String dbName, PermissionVo permissionVo) {
    if (permissionVo == null) {
      return false;
    }
    List<String> dataSource = permissionVo.getDataSource();
    if (dataSource == null || dataSource.size() != 3) {
      return false;
    }
    if (!"TABLE_OR_VIEW".equals(dataSource.get(0))) {
      return false;
    }
    if (!dbName.equalsIgnoreCase(dataSource.get(1))) {
      return false;
    }
    String tableName = dataSource.get(2);
    if (tableName == null || tableName.isEmpty()) {
      return false;
    }
    String action = permissionVo.getAction();
    if (action == null || action.isEmpty()) {
      return false;
    }
    action = action.toUpperCase();
    try {
      if (!"ADMIN".equals(action) && !GuardianSQLPrivilegeType.ALL_ON_TABLE.contains(GuardianSQLPrivilegeType.valueOf(action))) {
        return false;
      }
    } catch (Exception ex) {
      return false;
    }
    return true;
  }


getDataSource - 找一个有数据传输的组件，然后f12开开发者模式,找数据传输包，datasource
dbowner-有dbowner的接口，permvo里没有owner的权限
v2的重构->没什么可改的啊

hive预提交报错，我交了两次和以前一样的版本也报错，报错信息也没有很奇怪 ->hive那边代码改了，要记得常去rebase trunk那边的项目
ctrl+shift+N 搜索文件

gedit shortcuts for searching:

Ctrl + F                    Find a string.
Ctrl + G                    Find the next instance of the string.
Ctrl + Shift + G    Find the previous instance of the string.
Ctrl + K                    Interactive search.
Ctrl + H                    Search and replace.
提交了warp-39528,感觉没什么可改了的
重新创建git，在图片里有保存

1204 
解决apacheds中的show tables的迁移
在ds中过滤保留一个权限信息，然后hive直接调取ds中的信息
apacheds在哪里处理权限，并涉及到和和用户有权限的db，table信息
hive怎么调取apacheds的接口

LDAP（轻量级目录访问协议）以目录的形式来管理资源（域用户，用户组，地址簿，邮件用户，打印机等等）。
apacheds = apache directory server


679-692
      List<PermissionVo> permVos = guardianClient.userPermissions(username, component, InceptorPermUtil.convertDatabase(dbName), true);
      Set<String> tblsWithPrivsSet = new HashSet<>();
      if (permVos != null) {
        for (PermissionVo permVo : permVos) {
          if (!isValidTablePerm(dbName, permVo)) {
            continue;
          }
          String tableName = permVo.getDataSource().get(2).toLowerCase();
          tblsWithPrivsSet.add(tableName);
        }
      }
      List<String> tblsWithPrivs = new ArrayList<>(tblsWithPrivsSet);
      tbls.retainAll(tblsWithPrivs);
      return tbls;

permcontroller中引用了fortress的接口去审查权限
集群没开启
引用jar包要加classpath，project structure-add jar
hive端代码的rebase pom文件 
session问题
改代码-引pom-hive的逻辑metalist
vi跳到最后一行 shift+g

df-h查看磁盘空间
cd /var/log 看日志
ls -al
rm messages-201911* -rf

hive代码对标8.0
guardian代码对应master
跑代码fortress类不能引入项目->自己手写一个fortress类
ql跑不通，改了pom文件
打成jar包发送配置发现集群验证码拿不到
集群中还有未启动的服务

1205
236磁盘满了
service transwarp-manager restart 服务重启
看日志 [root@tw-node1237 agent]# kubectl get po -owide | grep inceptor
inceptor-metastore-inceptor1-bc6c6cc6b-vtsp9         1/1       Running            0          50m       172.16.1.237   tw-node1237
inceptor-metastore-inceptor1-bc6c6cc6b-vvjjr         1/1       Running            0          50m       172.16.1.238   tw-node1238
inceptor-metastore-slipstream1-7f8d6c8cbf-zxlsr      1/1       Running            0          20h       172.16.1.236   tw-node1236
inceptor-server-inceptor1-697b7f4665-zxxgs           0/1       CrashLoopBackOff   13         49m       172.16.1.237   tw-node1237
[root@tw-node1237 agent]# kubectl logs inceptor-server-inceptor1-697b7f4665-zxxgs

inceptor启动问题 用pcopydeps下所有的lib。打包解压到镜像

vim shift+g 到末尾 ctrl+z结束
guARDIAN filterTB不起作用 ->查日志 引入的文件有冲突 
[root@519cba665439 lib]# ls | grep asn
api-asn1-api-1.0.0-M20.jar
api-asn1-api-1.0.0-RC1.jar

要用ldap去查询调用 重写那边的接口用底层去作，在guardian server上绕过plugin去分缓存
首先就要先去了解guardian用ldap做的事情，然后分析可行性
permcontroller(userpermission)->permmanager(..)->reviermgr(search)->permp(finduserpermission)->permDao(findinheriteduserpermission)->searchresults->permDao(unloadpopldapentry)/ldapDataProvicer(search)->searchRequest/ladpCoreSessionConnection(search)/ladpnetworkconnection(search)->ldapcoresessionconnection(search)->coresession->defaultsession->
opertationManager(search)->entryfilteringcursor->referalmanager(getparentreferal)->dnnode(getelement)

1206


   /**
     * @param user
     * For this function filter.getInheritance not work
     * @return
     * @throws org.apache.directory.fortress.core.FinderException
     *
     */
    List<String> findUserAuthorizedDataNodes( User user, PermFilterParams filter ) throws FinderException
    {
        List<String> res = new ArrayList<>();
        LdapConnection ld = null;
        PermObj permObj = new PermObj( filter.getDataSource(), filter.getComponent() );
        int posn = permObj.getObjName().size();
        String permObjDn = getDn( permObj, user.getContextId() );

        try {
            StringBuilder filterbuf = new StringBuilder();
            filterbuf.append(GlobalIds.FILTER_PREFIX);
            filterbuf.append(PERM_OP_OBJECT_CLASS_NAME);
            filterbuf.append(")");
            // caseIgnoreSubstringsMatchSubtree extensible filter
            if ( StringUtils.isNotEmpty( filter.getSubString() ) )
            {
                String permObjVal = encodeSafeText(filter.getSubString(), GlobalIds.PERM_LEN);
                filterbuf.append("(");
                filterbuf.append(GlobalIds.POBJ_NAME);
                filterbuf.append(":dn:" + EXT_MATCH_RULE + ":");
                filterbuf.append("=");
                filterbuf.append(permObjVal);
                filterbuf.append(")");
            }
            // filter by action
            if ( StringUtils.isNotEmpty( filter.getAction() ) )
            {
                String permOpVal = encodeSafeText(filter.getAction(), GlobalIds.PERM_LEN);
                filterbuf.append("(");
                filterbuf.append(GlobalIds.POP_NAME);
                filterbuf.append("=");
                filterbuf.append(permOpVal);
                filterbuf.append(")");
            }
            // For inherited filter
            if ( filter.isInheritance() ) 
            {
                filterbuf.append("(|");
                Set<String> roles = RoleUtil.getInheritedRoles(user.getRoles(), user.getContextId());
                GroupDAO groupDAO = new GroupDAO();
                List<Group> groups = groupDAO.find(user);
                roles.addAll(findRolesByGroup(groups, user.getContextId()));

                if ( CollectionUtils.isNotEmpty(roles) )
                {
                    for (String uRole : roles) {
                        filterbuf.append("(");
                        filterbuf.append(ROLES);
                        filterbuf.append("=");
                        filterbuf.append(uRole);
                        filterbuf.append(")");
                    }
                }

                if ( CollectionUtils.isNotEmpty(groups) )
                {
                    for (Group group : groups) {
                        filterbuf.append("(");
                        filterbuf.append(GlobalIds.GROUP_NODES);
                        filterbuf.append("=");
                        filterbuf.append(group.getName());
                        filterbuf.append(")");

                    }
                }

                filterbuf.append("(");
                filterbuf.append(USERS);
                filterbuf.append("=");
                filterbuf.append(escapeLDAPSearchFilter(user.getUserId()));
                filterbuf.append(")))");
                ld = getAdminConnection(true);

                if (filter.getPageSize() >= 0) 
                {
                    String error = "Cannot search page in PermDAO.findUserAuthorizedDataNodes";
                    throw new FinderException(GlobalErrIds.PERM_USER_SEARCH_FAILED, error);
                }
                SearchCursor searchResults = search(ld, permObjDn,
                    filter.getScope(), filterbuf.toString(), PERMISSION_OP_ATRS, false, 0);

                while (searchResults != null && searchResults.next()) {
                    res.addAll(getNextLevelObjNameList(searchResults.getEntry().getDn(), posn));
                }

                // Find inherited permissions
                Set<String> inheritedGroups = GroupUtil.getInheritedGroups(groups, user.getContextId());

                if ( CollectionUtils.isNotEmpty(inheritedGroups) )
                {
                    filterbuf = new StringBuilder();
                    filterbuf.append(GlobalIds.FILTER_PREFIX);
                    filterbuf.append(PERM_OP_OBJECT_CLASS_NAME);
                    filterbuf.append(")");
                    // filter by action
                    if ( StringUtils.isNotEmpty(filter.getAction() ) ) 
                    {
                        String permOpVal = encodeSafeText(filter.getAction(), GlobalIds.PERM_LEN);
                        filterbuf.append("(");
                        filterbuf.append(GlobalIds.POP_NAME);
                        filterbuf.append("=");
                        filterbuf.append(permOpVal);
                        filterbuf.append(")");
                    }
                    filterbuf.append("(|");

                    for (String group : inheritedGroups) {
                        filterbuf.append("(&(");
                        filterbuf.append(GlobalIds.GROUP_NODES);
                        filterbuf.append("=");
                        filterbuf.append(group);
                        filterbuf.append(")(");
                        filterbuf.append(GlobalIds.HERITABLE_GROUP);
                        filterbuf.append("=");
                        filterbuf.append(group);
                        filterbuf.append("))");
                    }

                    filterbuf.append("))");

                    searchResults = search(ld, permObjDn,
                        filter.getScope(), filterbuf.toString(), PERMISSION_OP_ATRS, false, 0);
                    while (searchResults != null && searchResults.next()) {
                        res.addAll(getObjNameList(searchResults.getEntry().getDn()));
                    }
                }
            } else {
                // for non-inherited filter
                filterbuf.append( "(" );
                filterbuf.append( USERS );
                filterbuf.append( "=" );
                filterbuf.append( escapeLDAPSearchFilter( user.getUserId() ) );
                filterbuf.append( "))" );
                ld = getAdminConnection( true );

                SearchCursor searchResults;
                if ( filter.getPageSize() >= 0 ) 
                {
                    String error = "Cannot search page in PermDAO.findUserAuthorizedDataNodes";
                    throw new FinderException( GlobalErrIds.PERM_USER_SEARCH_FAILED, error);
                } else {
                    searchResults = search( ld, permObjDn,
                        filter.getScope(), filterbuf.toString(), PERMISSION_OP_ATRS, false, 0 );
                }

                while ( searchResults.next() )
                {
                    res.addAll(getNextLevelObjNameList(searchResults.getEntry().getDn(), posn) );
                }
            }
        }
        catch ( LdapException e )
        {
            String error = "User [" + user.getUserId()
                + "] caught LdapException in PermDAO.findUserAuthorizedDataNodes=" + e.getMessage();
            throw new FinderException( GlobalErrIds.PERM_USER_SEARCH_FAILED, error, e );
        }
        catch ( CursorException e )
        {
            String error = "User [" + user.getUserId()
                + "] caught CursorException in PermDAO.findUserAuthorizedDataNodes=" + e.getMessage();
            throw new FinderException( GlobalErrIds.PERM_USER_SEARCH_FAILED, error, e );
        }
        finally
        {
            closeAdminConnection( ld );
        }

        return res;
    }
git branch -D branchName git删除分支
git cherry-pick fd8d18a693cde46a0a72b818e5b95100b335d0cb
git commit -am

2497-2502 getDn(get the dn for the permobj)
2504-2584 
permList

每一个系统、协议都会有属于自己的模型，LDAP也不例外，在了解LDAP的基本模型之前我们需要先了解几个LDAP的目录树概念：

（一）目录树概念

1. 目录树：在一个目录服务系统中，整个目录信息集可以表示为一个目录信息树，树中的每个节点是一个条目。

2. 条目：每个条目就是一条记录，每个条目有自己的唯一可区别的名称（DN）。

3. 对象类：与某个实体类型对应的一组属性，对象类是可以继承的，这样父类的必须属性也会被继承下来。

4. 属性：描述条目的某个方面的信息，一个属性由一个属性类型和一个或多个属性值组成，属性有必须属性和非必须属性。

dc 域名的部分，其格式是将完整的域名分成几部分，如域名为example.com变成dc=example,dc=com（一条记录的所属位置）
uid 用户ID 
ou 组织单位，组织单位可以包含其他各种对象（包括其他组织单元）
cn 公共名称 如“Thomas Johansson”（一条记录的名称）
dn 一条记录的位置（唯一）uid=songtao.xu,ou=oa组,dc=example,dc=com”
rdn 相对辨别名，类似于文件系统中的相对路径，它是与目录树结构无关的部分 如“uid=tom”或“cn= Thomas Johansson”

1. encode安全編碼可防止惡意腳本輸入錯誤
  如果在傳遞到dao層之前未執行數據過濾，則可能發生這種情況。

2. 引package org.apache.directory.fortress.core.impl;

This utility wraps {@link org.apache.directory.fortress.core.impl.HierUtil} methods to provide hierarchical functionality for the {@link org.apache.directory.fortress.core.model.Role} data set.
 * The {@code cn=Hierarchies, ou=Roles} data is stored within a cache, {@link #roleCache}, contained within this class.  The parent-child edges are contained in LDAP,
 * in {@code ftParents} attribute.  The ldap data is retrieved {@link org.apache.directory.fortress.core.impl.RoleP#getAllDescendants(String)} and loaded into {@code org.jgrapht.graph.SimpleDirectedGraph}.
 * The graph...


該實用程序包裝{@link org.apache.directory.fortress.core.impl.HierUtil}方法，以為{@link org.apache.directory.fortress.core.model.Role}數據集提供分層功能。
 * {@code cn = Hierarchies，ou = Roles}數據存儲在此類中包含的緩存{@link #roleCache}中。父子邊緣包含在LDAP中，
 *在{@code ftParents}屬性中。檢索ldap數據{@link org.apache.directory.fortress.core.impl.RoleP＃getAllDescendants（String）}，然後將其加載到{@code org.jgrapht.graph.SimpleDirectedGraph}中。

     * For instance, if cn=example, dc=acme, dc=org is the Dn to check,
     * and if dc=acme, dc=org is a referral, this this method will return true.


permcontroller(userpermission)->permmanager(..)->reviermgr(search)->permp(finduserpermission)->permDao(findinheriteduserpermission)->searchresults->permDao(unloadpopldapentry)/ldapDataProvicer(search)->searchRequest/ladpCoreSessionConnection(search)/ladpnetworkconnection(search)->ldapcoresessionconnection(search)->coresession->defaultsession->
defaultopertationManager(search)->entryfilteringcursor->referalmanager(getparentreferal)->dnnode(getelement)


ldap:
传入的perm数据会在服务器端提取信息并通过这些信息（datasource和component）得到一个自己具有唯一可区别的名称Dn(getDn)，对应于ladp目录树的一个节点，保存在permObjDn中。
根据perm数据的prefix，substring，action形成一个过滤器字符串存在filterbuf中，scope和cookie单独作为过滤条件传到search部分
于此同时拉取存在缓存中的该用户对应的role和group权限，以及父子关系图，一并合并到filterbuf中
将所有的过滤信息构建一个searchcursor，每次查找找到符合条件的最上面一层（getParentReferral（dn))，形成一个查找域的查询链,在这个查找域里面把符合条件的当前节点解析到perm中，然后pop出去，进行下一轮查找，最后形成一个permlist
有继承的情况会包装成一个组，然后照上面的逻辑再进行一次perm的查询过滤。

data model
DIT是ldap的树，每个树上的节点叫entry，entry有名字，区别于同层节点的名字叫RDN，从特定节点到树的根的直接下级的节点的路径序列为DN
 An example of a Distinguished Name is CN=Steve Kille, O=Isode Limited, C=GB

 The concatenation of the relative distinguished names of the sequence of entries from a particular
 entry to an immediate subordinate of the root of the tree forms that entry's Distinguished Name (DN), which is unique in the tree. 

服务器会用缓存或者存储节点的信息查询或比较语句，如果改动的话返回referal？
Some servers may hold cache or shadow copies of entries, which can be used to answer search and comparison queries, but will return referrals or contact other servers if modification operations are requested.

节点属性
The following four attributes MUST be present in all subschema
   entries:

   - cn: this attribute MUST be used to form the RDN of the subschema
     entry.

   - objectClass: the attribute MUST have at least the values "top" and
     "subschema".

   - objectClasses: each value of this attribute specifies an object
     class known to the server.

   - attributeTypes: each value of this attribute specifies an attribute
     type known to the server.
referal
the referral field is present in an
   LDAPResult if the LDAPResult.resultCode field value is referral, and
   absent with all other result codes.
它包含对可以通过LDAP或服务器访问的另一台服务器（或一组服务器）其他协议。
http://www.hjp.at/doc/rfc/rfc2251.html
DN: ftOpNm=DELETE,ftObjNm=test,ftObjNm=TABLE_OR_VIEW,ou=inceptor1,ou=Permissions,ou=RBAC,dc=tdh
component：hive.metastore.service.id -> component eg. inceptor1->ou
datasource->list<String>->maps to 'ftObjNm' attribute in 'ftObject' object class.->ftobjnm
ftpermName

/**
     * Return the contextId for this record.  The contextId is used for multi-tenancy to isolate data sets within a particular sub-tree within DIT
     *
     * @return value maps to sub-tree in DIT, for example ou=contextId, dc=jts, dc = com.
     */
    public String getContextId()
    {
        return contextId;
    }


git rm -r –cached filePath unversioned file?

1209
1. WARP-31985: 修改相关问题的改动，重新配置集群测试，并提交代码
2. WARP-39528: 增加跨域互信的token认证方式，提交到guardian-3.1，重构相应部分的代码
3. review Apache ds中关于ldap过滤保留权限信息的部分

ldap:
传入的perm数据会在服务器端提取信息并通过这些信息（datasource和component）得到一个自己具有唯一可区别的名称Dn(getDn)，对应于ladp目录树的一个节点，保存在permObjDn中。
根据perm数据的prefix，substring，action形成一个过滤器字符串存在filterbuf中，scope和cookie单独作为过滤条件传到search部分
于此同时拉取存在缓存中的该用户对应的role和group权限，以及父子关系图，一并合并到filterbuf中
将所有的过滤信息构建一个searchcursor，每次查找找到符合条件的最上面一层（getParentReferral（dn))，形成一个查找域的查询链,在这个查找域里面把符合条件的当前节点解析到perm中，然后pop出去，进行下一轮查找，最后形成一个permlist
有继承的情况会包装成一个组，然后照上面的逻辑再进行一次perm的查询过滤。
ftOpNm=select,ftObjNm=testtb,ftObjNm=test,ftObjNm=TABLE_OR_VIEW,ou=inceptor1,ou=permissions,ou=RBAC,dc=tdh

permcontroller(userpermission)->permmanager(..)->reviermgr(search)->permp(finduserpermission)->permDao(findinheriteduserpermission)->searchresults->permDao(unloadpopldapentry)/ldapDataProvicer(search)->searchRequest/ladpCoreSessionConnection(search)/ladpnetworkconnection(search)->ldapcoresessionconnection(search)->coresession->defaultsession->
defaultopertationManager(search)->defaultoperationManager-->defaultoperationmanager(search)->inceptor(search)->next->baseIntercptor(next/search)->apacheds项目Btree/lmdbPartition(search)->defaulltSearchEngine(computeresult)
cursorBuilder(Build)

->entryfilteringcursor->referalmanager(getparentreferal)->dnnode(getelement)
>buildReferralExceptionForSearch->interceptor search->AciAuthorizationInterceptor

跟踪filter的一些字段
1. GlobalIds.FILTER_PREFIX 
    /**
     * This string literal contains a common start for most ldap search filters that fortress uses.
     */
    public static final String FILTER_PREFIX = "(&(" + SchemaConstants.OBJECT_CLASS_AT + "=";
2. PERM_OP_OBJECT_CLASS_NAME
 private static final String PERM_OP_OBJECT_CLASS_NAME = "ftOperation";
3.  POBJ_NAME    
    /**
     * Attribute name for storing Fortress permission object names.
     */
    public static final String POBJ_NAME = "ftObjNm";

guaridan-plugin -> guardian(perm controller) -> apacheds(ldap)
                  ->     ->     ->    ->   ->
 归为两个问题，
1. 如果是想改进ldap的存储结构来实现树形目录只查到某一层的效果 是不可取的，因为某一库上的权限肯定要查到表层，表上的权限ds端的ldap本身已经优化很好了，很少优化的空间 ->
DIT->(RBAC->inceptor1)->(Table_or_view->test->testtb)->admin/select
能不能我们查表,甚至查库的时候，不继续向下查询，直接在下一节点查到ftusers并返回->这个user层会不会很多/会记录很多重复数据


2. 如果是想绕开guardian后端，用plugin直接和apacheds交互来实现减少消耗，已经实践过了
sub1.要引入很多包 sub2.会破坏项目组织结构 controller-manager层

1210 
改动：apacheds端拉取权限文件变小
打开隐藏文件 ctrl+h
git无法使用 permission denied
which git 查找git路径 配置git路径
git branch -v 
git remote -v 配置远程仓库信息

 while ( searchResults.next() )
            {
                res.addAll(getObjNameList(searchResults.getEntry().getDn()));
            }

a项目和b项目同用一个git，a项目改动让b也反应
mvn install -DskipTests
先拉缓存

guardianserver的jar包要用guardian的镜像
guardian-boot-server要改很多
服务起不来 看日志 /var/logs

原因：
2019-12-10 16:27:15,365 ERROR exec.DDLTask: (DDLTask.java:execute(786)) [HiveServer2-Handler-Pool: Thread-218(SessionHandle=cafffb05-e72f-4569-8fd7-a0b12f708be5)] - java.lang.NoSuchMethodError: io.transwarp.guardian.client.GuardianClient.userDatabasePerms(Ljava/lang/String;Ljava/lang/String;Ljava/util/List;ZLjava/lang/String;)Ljava/util/List;
NoSuchMethodError:路径错了
find / -name 'guardian-client*' 查找文件

1211
1. 方法没找到，是针对程序而言的，不用被自己误导（要以JVM的思维去考虑），我们明明已经写了方法，检查过也都没问题，但是就是报NoSuchMethodError 。这里需要强调一下，方法是写在类里面的，方法找不到，可能是类的原因。
找到Integer.class所在的jar包方法

出现(Ljava/lang/String;Ljava/lang/String;)Ljava/util/List;

     表明是数组处理出了问题；

2、由于Webservice不能传递List，所以接收Java端列表数据集合时，要注意在java端

     转化为数组形式传递之；

3、这个问题出现主要是java里面的List的泛型出现问题；

     关于List<T>,可以参照网上的资料；

4、本人检查自己代码，发现java服务端获取数组list之后，确实转化为Array[]了，但是

     仍然出现了上面的问题，调试了半天发现自己使用List时没有指定类型，只是如：

         List  ls = pdao.searchCategory(category);

     尝试加上类型：

         List<Product>  ls = pdao.searchCategory(category);

     将其他类似的地方全部加上强类型；这样.net端调用就不会出现这个问题；

The method works if you provide an array. The output of

String[] helloWorld = {"Hello", "World"};
System.out.println(helloWorld);
System.out.println(Arrays.toString(helloWorld));
is

[Ljava.lang.String;@45a877
[Hello, World]
jar -cf hello.jar

當 WCF 的服務器端函數 (Operation) 的返回類型爲 List<string> 時，實際返回的類型爲 string[] 數組，因此客戶端若仍用 List<string> 的變量去接收和賦值時，在編譯時期，即會發生下圖 1 的轉型錯誤：

find / -name 'guardian-client*' 查找文件

find . -name "*.jar"| awk '{print "jar -tf "$1}'| sh -x | grep -i "/GuardianClient.class"

+ jar -tf ./inceptor/lib/hbase-shaded-server-1.3.1-transwarp-6.2.0.jar
io/transwarp/guardian/client/GuardianClient.class

inux删除目录很简单，很多人还是习惯用rmdir，不过一旦目录beeline
非空，就陷入深深的苦恼之中，现在使用rm -rf命令即可。

直接rm就可以了，不过要加两个参数-rf 即：rm -rf 目录名字

-r 就是向下递归，不管有多少级目录，一并删除

-f 就是直接强行删除，不作任何提示的意思

1.guardian登不上->修改guardian-boot会导致前端页面没了
2.404报错->路径错乱

拦截器是基于java反射机制的，而过滤器是基于函数回调。 拦截器不依赖于Servlet容器，而过滤器依赖于servlet容器。 拦截器只能对action请求起作用，而过滤器可以对几乎所以的请求起作用。 拦截器可以访问action上下文，值栈里的对象，而过滤器不能。

拉镜像，删除镜像，看日志
config.ldif

guardianclient
1237/etc/guardian/conf 日志

1212
用ACL控制授权

我们在LDAP中创建目录树后,最感兴趣的就是如何控制用户在目录树中的权限(读写)。谁在什么条件下有记录权限，我们有权限看到哪些信息。ACL（Access Control List）访问控制列表就是解决用户权限问题的。
我们要把ACL写在哪里？
ACL写在OpenLDAP的服务端全局配置文件slapd.conf中

os当我们读取一个文件时，实际上是在目录中找到了这个文件的inode编号，然后根据inode的指针，把数据块组合起来，放入内存供进一步的处理。当我们写入一个文件时，是分配一个空白inode给该文件，将其inode编号记入该文件所属的目录，然后选取空白的数据块，让inode的指针指像这些数据块，并放入内存中的数据。

vi
你在命令模式下敲斜杆( / )这时在状态栏（也就是屏幕左下脚）就出现了 “/” 然后输入你要查找的关键字敲回车就可以了。 
如果你要继续查找此关键字，敲字符 n 就可以继续查找了。

对象类（ObjectClass）、属性类型（AttributeType）、语法（Syntax）分别约定了条目、属性、值，他们之间的关系如下图所示。所以这些构成了模式(Schema)，模式中的每一个元素都有唯一的OID编号，如2.5.4.41.条目数据在导入时通常需要接受模式检查，它确保了目录中所有的条目数据结构都是一致的。


在上图所示的树形结构中，树的根结点是一个组织的域名（dlw.com），其下分为3个部分，分别是managers、people和group，可将这3个组看作组织中的3个部门：如managers用来管理所有管理人员，people用来管理登录系统的用户，group用来管理系统中的用户组。当然，在该图中还可继续增加其他分支。
对于图中所示的树形结构，使用关系数据库来保存数据的话，需要设置多个表，一层一层分别保存，当需要查找某个信息时，再逐层进行查询，最终得到结果。
若使用目录来保存该图中的数据，则更直观。图中每个结点用一个条目来保存，不同类型的结点需要保存的数据可能不同，在LDAP中通过一个称为objectClass的类型来控制不同结点需要的数据（称为属性）。

对于目录中的数据怎样进行引用呢？前面提到过，每一个条目都有一个dn，因为dn是唯一的，因此就可找到需要结点的数据。dn的构造方式如下：
首先得到条目自己的名称（rdn，称为相对dn），然后开始向上逐级查找父结点，一直到根项为止。例如，对于图1-1中最右下方的结点，其dn为：
dn: cn=ldap, ou=group, o=dlw.com
通过这样的方式，即可唯一标识每一个结点。在现实生活中，有很多这种树形结构的数据，如计算机文件系统的目录结构、Internet中的域名等。这些类型的数据，只要不需要频繁的更新，都适合用目录来保存。

优化器根据数据库索引中的扫描计数，来确定最佳搜索过滤器执行路径。它通过在节点中设置count键值来注释表达式子树的节点。其目标是用计数来注释节点，以指示要迭代的节点最小化搜索中的数字周期。 SearchEngine依靠这些计数标记来确定适当的路径。

MySQL数据库的查询优化器使用了基于代价的查询执行计划估算，所以依赖于被查对象的各种数据，而数据是动态变化的，如表的元组数。如果实时获取这些数据，系统计算的开销会比较大。为了避免这样的问题，定期或者根据需要统计这些数据，则比较切合实际。优化器在物理优化阶段，需要对单表读取开销，两表连接开销，多表连接顺序开销等进行比较，比较基于的就是一些基础数据的值，这些数据通常不会被实时更新，所以优化器有时做出的计划未必是最合适的。

/var/logs

    public EntryFilteringCursor search( SearchOperationContext searchContext ) throws LdapException
    {
        return null;
    }



=* is an old way to write right outer joins. For example:

select  *
from    A
right outer join
        B
on      A.bid = B.id

Is written in the old style like:

select  *
from    A
,       B
where   A.bid =* B.id

1213
重构项目改动部分/把ldap部分写成wiki
删除镜像docker image rmi ID
Error response from daemon: conflict: unable to delete 3c9ab0cb34db (must be forced) - image is referenced in multiple repositories

强制删除-f
image has dependent child images
https://www.cnblogs.com/111testing/p/11208086.html


重构getAuthorizedResources
getAuthorizedDatasources

SQL 表别名
在 SQL 语句中，可以为表名称及字段（列）名称指定别名（Alias），别名是 SQL 标准语法，几乎所有的数据库系统都支持。通过关键字 AS 来指定。
表别名语法：

SELECT column FROM table AS table_alias


 We want the Evaluator with the smallest scan count first
         * since this node has the highest probability of failing, or
         * rather the least probability of succeeding.  That way we
         * can short the sub-expression evaluation process.

https://tools.ietf.org/html/rfc4512#section-4.1


LMDB的全称是LightningMemory-Mapped Database，闪电般的内存映射数据库。它文件结构简单，一个文件夹，里面一个数据文件，一个锁文件。数据随意复制，随意传输。它的访问简单，不需要运行单独的数据库管理进程，只要在访问数据的代码里引用LMDB库，访问时给文件路径即可。、

对象类（ObjectClass）是属性的集合，LDAP预想了很多人员组织机构中常见的对象，并将其封装成对象类。

对象类（ObjectClass）、属性类型（AttributeType）、语法（Syntax）分别约定了条目、属性、值。这些构成了模式(Schema)，模式中的每一个元素都有唯一的OID编号

1216
工作周报 - 李镇邦 20191209 ~ 20191213

完成：
1. WARP-31985: 减小查询权限时apacheds端拉取权限文件，增加传输效率

进行中：
1. 迁移WARP-31985接口到重构版本
2. ldap与lmdb的底层搜索逻辑代码

本周：
1. 修改ldap与lmdb的底层搜索逻辑代码，增加查询效率

mmap和零拷贝

关于guardian的权限访问路径

一，

                        ldap   
fortress(通过ldap RBAC)--------apacheds--lmdb
客户端调用guardian-plugin的接口访问guardian的permmanager接口接入fortress。

fortress
LDAP提供基于角色的访问控制，委派管理和密码策略服务。

lmdb
基于文件映射IO（mmap）
基于B+树的key-value接口
基于MVCC（Multi Version Concurrent Control）的事务处理
类bdb（berkeley db）的api

apacheds
Java编写的可嵌入目录服务器

二，
permDAO(findInheritedUserPermissions)
传入参数：user, filter
将filter中的datasource和component信息提取得到我们在ldap上我们要搜索范围的dn
filter的其他过滤信息添加到filterbuf中，searchresult通过searchCursor保存
搜索接口为search, 返回后节点返回到unloadpopldapEntry，放到permlist中 然后返回

LdapDataProvider(search)
传入的参数都放在searchRequest中配置ldap报文 //能不能减少属性字段 传输到后端的apacheds？
可以配置不同的sort方法（sortkey）

ldapConnection(Search),ldapCoreSessionConnection(Search)
传入参数searchrequest
原子序列号自增
进入session的search，
返回EntryToResponseCursor //可以像searchHandler一样 在这设置size和时间限制

coreSession/DefaultCoreSession(search)
传入参数searchrequest
补充：ldap目录结构
1. 条目（Entry） 条目（Entry）就是目录管理的对象，他是LDAP中最基本的节点，算是数据库中的记录。
2. 属性（Attribute）每个条目都可以有很多属性（Attribute），比如常见的人都有姓名、地址、电话等属性。每个属性都有名称及对应的值，属性值可以有单个、多个。比如ftusers
3. 属性类型 每个属性都有唯一的属性类型（AttributeType），属性类型约定属性值的数据格式和语法类型（Syntax）。类型也规定了属性查询时的匹配规则、排序顺序、大小写敏感等。比如syntax，matching rules
4. 对象类 对象类（ObjectClass）是属性的集合，LDAP预想了很多人员组织机构中常见的对象，并将其封装成对象类。比如，top
5. 模式 对象类（ObjectClass）、属性类型（AttributeType）、语法（Syntax）分别约定了条目、属性、值，每个entry都有must和may属性，每个属性具有不同的值并附带自己的属性类型。条目中可能包含一些属性是在对象类1中，有些是对象类2中。这些全部构成了模式(Schema)，模式中的每一个元素都有唯一的OID编号，如2.5.4.41.条目数据在导入时通常需要接受模式检查，它确保了目录中所有的条目数据结构都是一致的。
cursor移动走的是哪个路径？
返回cursor

operationManager/defaultOperationManager(search)
输入context，获取dn，获取ds中预定义的interceptor拦截器
进入search,把请求推给拦截器链，保护server的多线程安全
返回cursor

interceptor(search)
补充：apacheds的interceptor
每个Interceptor的调用会依据它的声明顺序依次执行，而且最先执行的都是Interceptor中的preHandle方法，所以可以在这个方法中进行一些前置初始化操作或者是对当前请求的一个预处理，也可以在这个方法中进行一些判断来决定请求是否要继续进行下去。该方法的返回值是布尔值Boolean 类型的，当它返回为false时，表示请求结束，后续的Interceptor和Controller都不会再执行；当返回值为true时就会继续调用下一个Interceptor 的preHandle 方法，如果已经是最后一个Interceptor 的时候就会是调用当前请求的Controller 方法。
apacheds总共有14个intercaptor，search一共7个，每个Interceptor的调用会依据它的声明顺序依次执行
Let's consider the search operation. It will be processed successively by the following Interceptors, as it can be deduced by the two previous tables :

    NormalizationInterceptor
    AuthenticationInterceptor
    AciAuthorizationInterceptor
    DefaultAuthorizationInterceptor
    SchemaInterceptor
    OperationalAttributeInterceptor
    SubentryInterceptor

We can do the same exercise for each operation.

baseinterceptor(search)
此处有争议
进入nexus的search
返回cursor

lmdbPartition(search)
传入searchcontext，建立读事务，
通过searchEngine的computeResult得到计算结果，包装成filteringCursor返回


defaultSearchEngine(ComputeResult)
传入schemaManager, searchContext
从seachContext中得到搜索范围，baseDn和filter
从basedn得到uuid
准备返回的数据结构searchResult和顺序集
空节点返回
得到aliase的base

replaceExtensibleNode


object level的搜索：
db.fetch/optimizer/设置evaluator装入searchResult并返回

其他搜索：
得到一个scopenode，用这个节点设置optimizer(设置表达式优化）和evaluator（选择mathing rule匹配的过滤器），
判断原有的candidateset

进入optimizesorting
dn长度是可知的，用dn长度排序得到的节点
dn是由rdns组成的，rdn是一个list的数据结构，可以直接返回size
设定所有子节点中含有这一属性索引的节点，并且按照dn长度排序
有一个todo，不需要知道dn来知道dnsize

返回void到compute result//已经排序
或全表搜索，遍历cursor时用evaluator判断matchingrule删选
返回searchresult

路径
1. defaultSearchEngine(annotate)-defaultoptimiser(getfullscan)->store(getIndex)->lmdbPartition(getIndex)->lmdbPartiotion(doinit)//优化查询语句

2. defaultOptimiser(getScan)->index(forwardValueCursor)->lmdbIndex->lmdbTaable->lmdbValueCursor->Cursor->store(seek)->mdb_cursor_get  //遍历并返回


3. defaultSearchEngine(cursorBuilder.build)->index(forwardCursor)->lmdbTable(cursor)->lmdbCursor(getlmdbcursor)->database(opencursor)//通过cursor排序并优化

defaultSearchEnngine(evaluator.build)->

4. equalityCursor(previors/next)->extensibleEvaluator(evaluate)->extensibleEvaluator(dn)//每次移动时都会执行判断evaluator

5. optimizeSorting->


optimizer->优化搜索语句
evaluator->根据matchingrule的过滤器
cursor确定满足条件的节点
optimizeSorting根据sortkey制定的结果排序（index，dnsize）









=====







传入的perm数据会在服务器端提取信息并通过这些信息（datasource和component）得到一个自己具有唯一可区别的名称Dn(getDn)，对应于ladp目录树的一个节点，保存在permObjDn中。
根据perm数据的prefix，substring，action形成一个过滤器字符串存在filterbuf中，scope和cookie单独作为过滤条件传到search部分
于此同时拉取存在缓存中的该用户对应的role和group权限，以及父子关系图，一并合并到filterbuf中
将所有的过滤信息构建一个searchcursor，每次查找找到符合条件的最上面一层（getParentReferral（dn))，形成一个查找域的查询链,在这个查找域里面把符合条件的当前节点解析到perm中，然后pop出去，进行下一轮查找，最后形成一个permlist
有继承的情况会包装成一个组，然后照上面的逻辑再进行一次perm的查询过滤。


SpringMVC中的Interceptor同Filter一样都是链式调用。每个Interceptor的调用会依据它的声明顺序依次执行，而且最先执行的都是Interceptor中的preHandle方法，所以可以在这个方法中进行一些前置初始化操作或者是对当前请求的一个预处理，也可以在这个方法中进行一些判断来决定请求是否要继续进行下去。该方法的返回值是布尔值Boolean 类型的，当它返回为false时，表示请求结束，后续的Interceptor和Controller都不会再执行；当返回值为true时就会继续调用下一个Interceptor 的preHandle 方法，如果已经是最后一个Interceptor 的时候就会是调用当前请求的Controller 方法。


代码
permcontroller(userpermission)->permmanager(..)->reviermgr(search)->permp(finduserpermission)->permDao(findinheriteduserpermission)->searchresults->permDao(unloadpopldapentry)/ldapDataProvicer(search)->searchRequest/ladpCoreSessionConnection(search)/ladpnetworkconnection(search)->ldapcoresessionconnection(search)->coresession->defaultsession->
defaultopertationManager(search)->defaultoperationManager-->defaultoperationmanager(search)->inceptor(search)->next->baseIntercptor(next/search)->apacheds项目Btree/lmdbPartition(search)->defaulltSearchEngine(computeresult)
cursorBuilder(Build)


1. defaultSearchEngine(annotate)-defaultoptimiser(getfullscan)->store(getIndex)->lmdbPartition(getIndex)->lmdbPartiotion(doinit)//优化查询语句

2. defaultOptimiser(getScan)->index(forwardValueCursor)->lmdbIndex->lmdbTaable->lmdbValueCursor->Cursor->store(seek)->mdb_cursor_get  //遍历并返回


3. defaultSearchEngine(cursorBuilder.build)->index(forwardCursor)->lmdbTable(cursor)->lmdbCursor(getlmdbcursor)->database(opencursor)//通过cursor排序并优化

defaultSearchEnngine(evaluator.build)->

4. equalityCursor(previors/next)->extensibleEvaluator(evaluate)->extensibleEvaluator(dn)//每次移动时都会执行判断evaluator

5. optimizeSorting->


Let's consider the search operation. It will be processed successively by the following Interceptors, as it can be deduced by the two previous tables :

    NormalizationInterceptor
    AuthenticationInterceptor
    AciAuthorizationInterceptor
    DefaultAuthorizationInterceptor
    SchemaInterceptor
    OperationalAttributeInterceptor
    SubentryInterceptor

indice索引
資料指標(Data Cursor) 或稱游標，是在資料庫引擎 (Database Engine)中，讓開發人員或資料庫管理員可以遍歷、瀏覽檢索結果的資料列(稱為資料查詢結果集, Result set)，是主要用於在結果集中移動到某一資料列(row)的控制結構。游標可以被看作是指向一組列中，代表某一列的指針。游標一次只能引用一列，但可以根據需要移動到結果集的其他列。 僅前移型指標 (Forward-Only Cursor) 是一旦將指標往前移時，其走過的指標之前的結果集就會被捨棄，因此應用程式不能再往後移動指標，但也因此讓伺服器只需要記住指標在結果集中目前的位置即可，這讓它消耗的資源只有指標而已

cursor-transaction-mvcc\

dn是由rdns组成的，rdn是一个list的数据结构，可以直接返回size

客户端调用guardian-plugin的接口访问guardian的permmanager接口接入fortress
fortress:过LDAP提供基于角色的访问控制，委派管理和密码策略服务。


1217
因为lmdb的attribute的index是使用attribute的值进行排序的，因此可以借助这个排序来优化serverside的排序速度，如果ordering match允许的话则使用index加速排序过程
资料：1. https://ldapwiki.com/wiki/UUID
2.https://directory.apache.org/apacheds/basic-ug/2.2.1-simple-search.html
3. http://www.lmdb.tech/doc/group__mdb.html


       // Annotate the node with the optimizer and return search enumeration.
        optimizer.annotate( root );
        Evaluator<? extends ExprNode> evaluator = evaluatorBuilder.build( root );

        Set<String> uuidSet = new HashSet<String>();
        searchResult.setAliasDerefMode( aliasDerefMode );
        searchResult.setCandidateSet( uuidSet );

        long nbResults = cursorBuilder.build( root, searchResult );

        LOG.debug( "Nb results : {} for filter : {}", nbResults, root );

        if ( nbResults < Long.MAX_VALUE )
        {
            // handle sort request earlier
            optimizeSorting( schemaManager, searchContext, searchResult, resultSet );

            for ( String uuid : uuidSet )
            {
                IndexEntry<String, String> indexEntry = new IndexEntry<String, String>();
                indexEntry.setId( uuid );
                resultSet.add( indexEntry );
            }
        }
lmdb使用mmap, 同时在创建env对象时，数据库已经被整个映射进整个进程空间，因此系统在映射时，会给数据库文件保留全部地址空间，从而在根据上述算法获取真实数据库，系统触发缺页错误，进而从数据文件中获取整个页面内容。
游标对象是进行所有数据库操作的对象，读写都是基于游标进行。进行读写操作时，首先需要根据条件确定页面位置，从而获得一个游标，应用程序根据游标对象操作数据库。

lmdb代码主要分为page管理和cursor操作两块实现b-tree结构.
mdb_page_search_root: 从B-Tree根节点检索，根据key的值，从根节点开始遍历子树获取每一层对应的page，在page之内检索key，再根据B-Tree查找方法确定下一层子节点的page，层层遍历，从而最终确定key的位置或者判断 B-Tree中没有对应的key。同时将页面存放到cursor页堆栈中。这样cursor将可以重用对应的页面，为后续进行更新等操作提供便利。


1.getDescendantResources查询效率提升，现在是迭代
2.晚上修改wiki

resourceStorage/getDescendantResources -> resourceDaoImpl/getDescendantResources -> ResourceManager/getDescendantResources -> ResourceController/getDescendantResources

疑问1为什么要重构->apacheds主从切换问题,master节点挂了 不能做到容灾（zk和paxos协议都试过，估计是ldap学习难度比较大，referal没有合适的文档解析）
3. 为什么void会有返回值->reslut在函数里有变动
2迭代改成队列 先进先出？

CRC校验实用程序库 在数据存储和数据通讯领域，为了保证数据的正确，就不得不采用检错的手段。在诸多检错手段中，CRC是最著名的一种。CRC的全称是循环冗余校验。

ldap相对于rdb的好处
1. 通过按目录（层次结构）分组易于管理（由于树结构，可以将管理委派给每个分支）
2. 搜索性能良好，因为只需要搜索目标用户信息所在的目录（由于它是树结构，因此只搜索该分支而不搜索所有分支）。
3. 因为主要处理基于文本的信息，所以处理负荷很小。
4. 属性定义为搜索协议，也可以自己将属性添加为LDAP模式，作为自定义


  public static void getDescendantResources(ResourceServiceMapper resourceServiceMapper, ResourceMapper resourceMapper,
                                            ResourceVo resourceVo, List<ResourceVo> existedResources) {
    List<ResourceNode> childNodes = getChildNodes(resourceServiceMapper, resourceMapper, resourceVo);
    if (childNodes == null) {
      return;
    }
    for (ResourceNode node : childNodes) {
      ResourceVo newResource = DataSourceUtil.isSearchByPrefix(resourceVo) ?
          resourceVo.service().addNode(node.getType(), node.getValue()).build() :
          resourceVo.asParent().addNode(node.getType(), node.getValue()).build();
      Resource resource = ResourceStorage.getResource(resourceServiceMapper, resourceMapper, newResource);
      // resource ending with this node may not exist
      if (resource != null) {
        newResource.setId(resource.getId());
        newResource.setExternalId(resource.getExternalId());
        existedResources.add(newResource);
      }
      getDescendantResources(resourceServiceMapper, resourceMapper, newResource, existedResources);
    }
  }

1218
resourcestorage查询子节点路径查询
清本地缓存 或在mvn install原项目
df-h查看磁盘空间
cd /var/log 看日志
ls -al
rm messages-201911* -rf
清目录 重启  service transwarp-manager restart 服务重启
search的WARP-34954重构
主机名添加到/etc/hosts中

1219
search重构datasource资源
[{"component":"search1","dataSource":["GLOBAL"/"CLUSTER"]}]
"permissionVo":{"component":"search1","dataSource":["GLOBAL"],"action":"ADMIN","heritable":false,"grantable":false,"administrative":false}},{"name":"elasticsearch","principalType":"USER","permissionVo":{"component":"search1","dataSource":["GLOBAL"],"action":"ADMIN","heritable":false,"grantable":false,"administrative":false}},{"name":"public","principalType":"ROLE","permissionVo":{"component":"search1","dataSource":["GLOBAL"],"action":"ACCESS","heritable":false,"grantable":false,"administrative":false}}]

sionVo":{"component":"search1","dataSource":["CLUSTER"],"action":"CREATE_INDEX","heritable":false,"grantable":false,"administrative":false}}]


"permissionVo":{"component":"hdfs1","dataSource":["PATH","/","slipstreamstudio1","algorithm_jar"],"action":"ADMIN","heritable":false,"grantable":false,
"administrative":false}},{"name":"slipstream","principalType":"USER","permissionVo":{"component":"hdfs1","dataSource":["PATH","/","slipstreamstudio1","algorithm_jar"],"action":"WRITE","heritable":false,"grantable":false,
"administrative":false}},{"name":"slipstream","principalType":"USER","permissionVo":{"component":"hdfs1","dataSource":["PATH","/","slipstreamstudio1","algorithm_jar"],"action":"EXECUTE","heritable":false,
"grantable":false,"administrative":false}},{"name":"slipstream","principalType":"USER","permissionVo":{"component":"hdfs1","dataSource":["PATH","/","slipstreamstudio1","algorithm_jar"],"action":"READ","heritable":false,"grantable":false,"

resourceManager是做什么的 怎么获取datasource的信息并组装 resourceentry search相关

@Scope，也称作用域，在 Spring IoC 容器是指其创建的 Bean 对象相对于其他 Bean 对象的请求可见范围。在 Spring IoC 容器中具有以下几种作用域：基本作用域（singleton、prototype），Web 作用域（reqeust、session、globalsession），自定义作用域。


{
  "name": "admin",
  "permissionVo": {
    "action": "WRITE",
    "administrative": false,
    "component": "search1",
    "dataSource": [
      "INDEX","test1"
    ],
    "grantable": false,
    "heritable": false
  },
  "principalType": "USER"
}

Bean instantiation via constructor failed
Spring在实例化这个类的时候，先执行静态方法，此时某个类还未实例化(检查是否已加注解)，故而报了这个空指针错误。


{
  "serviceName": "search1",
  "serviceStatus": "ONLINE",
  "serviceType": "ELASTICSEARCH"
}

used by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'searchResourceMgr' defined in URL [jar:file:/home/transwarp/Downloads/work/guardian-backend/guardian/examples/target/lib/resource-manager-guardian-3.2.0.jar!/io/transwarp/guardian/resource/search/SearchResourceMgr.class]: Could not resolve matching constructor (hint: specify index/type/name arguments for simple parameters to avoid type ambiguities)
solve:-Pcopy-deps
resource/register
{
  "clusterName": "string",
  "configs": {},
  "description": "string",
  "lastHeartbeatTimestamp": 0,
  "offlineTimestamp": 0,
  "serviceHosts": [
    "string"
  ],
  "serviceName": "search1",
  "serviceStatus": "ONLINE",
  "serviceType": "ELASTICSEARCH",
  "timestamp": 0
}

resource/lookup
{
  "resourceName": "index",
  "resources": {},
  "serviceName": "search1",
  "serviceType": "ELASTICSEARCH",
  "userInput": "*"
}

1220
NPE 异常

NPE是指编程语言中的空指针异常
NullPointerException
编程语言中的空指针异常。
当应用程序试图在需要对象的地方使用 null 时，抛出该异常。这种情况包括：

    调用 null 对象的实例方法。
    访问或修改 null 对象的字段。
    将 null 作为一个数组，获得其长度。
    将 null 作为一个数组，访问或修改其时间片。
    将 null 作为 Throwable 值抛出。

1. 由于 HashMap 的干扰，很多人认为 ConcurrentHashMap 是可以置入 null 值，而事实上，存储null 值时会抛出 NPE 异常。

2. 注意 Math.random() 这个方法返回是 double 类型，注意取值的范围 0≤x<1（能够取到零值，注意除零异常），如果想获取整数类型的随机数，不要将 x 放大 10 的若干倍然后取整，直接使用 Random 对象的 nextInt 或者 nextLong 方法。

3. try 块中的 return 语句执行成功后，并不马上返回，而是继续执行 finally 块中的语句，如果此处存在 return 语句，则在此直接返回，无情丢弃掉 try 块中的返回点。 

caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'searchResourceMgr' defined in URL [jar:file:/home/transwarp/Downloads/work/guardian-backend/guardian/examples/target/lib/resource-manager-guardian-3.2.0.jar!/io/transwarp/guardian/resource/search/SearchResourceMgr.class]: Could not resolve matching constructor (hint: specify index/type/name arguments for simple parameters to avoid type ambiguities)
没有-Pcopy-deps导致example的启动路径没有执行

Returns a set of child node names for the given DataSource. The user should have  permission on each subtree node of the current node.

userDatasourcePerms


\   * Returns a set of child node names for the given DataSource.
   * The user should have permission on each subtree node of the current node.

throw new FinderException( GlobalErrIds.PERM_USER_SEARCH_FAILED, error, e );


userAuthorizedDataNodes
searchDataNodes

1223
1.分页/cookie->不传的话，就是变量对应类的默认初始值，builder模式就是适用于有非常多的变量，但是每一个变量又都不是必填的情况
2.客户端怎么改->就那么改 改成null就行其他不管
3.if语句的异常
private void checkElementIndex(int index) {
        if (!isElementIndex(index))
            throw new IndexOutOfBoundsException(outOfBoundsMsg(index));
    }

builder模式

user类中构造函数来传参数代码可读性很差，调用者很艰难

gettersetter方法 对象可能产生不一致状态，不可变类好处很差

Builder模式使用了链式调用。可读性更佳。
Builder的内部类构造方法中只接收必传的参数，并且该必传的参数适用了final修饰符
Builder模式拥有其所有的优点，而没有上述方法中的缺点。客户端的代码更容易写，并且更重要的是，可读性非常好。唯一可能存在的问题就是会产生多余的Builder对象，消耗内存。然而大多数情况下我们的Builder内部类使用的是静态修饰的(static)，所以这个问题也没多大关系。
由于Builder是非线程安全的，所以如果要在Builder内部类中检查一个参数的合法性，必需要在对象创建完成之后再检查。
建造者模式(Builder Pattern)：将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。
public User build() {
  User user = new user(this);
  if (user.getAge() > 120) {
    throw new IllegalStateException(“Age out of range”); // 线程安全
  }
  return user;
}



要好好看之前写的代码
不需要的逻辑不要加

.context.annotation.ConflictingBeanDefinitionException: Annotation-specified bean name 'resourceServiceController' for bean class 
Annotation-specified bean name 'resourceServiceController' for bean class [io.transwarp.guardian.server.boot.controller.ResourceServiceController] conflicts with existing, non-compatible bean definition of same name and class [io.transwarp.guardian.server.boot.controller.v2.ResourceServiceController]

mvn install -DskipTests -Pcopy-deps



du -h --max-depth=1 定位最大文件
cd /var/log 看日志
ls -al
rm messages-201911* -rf

ls –lhS 将文件以从大到小顺序展现

inceptor-server-inceptor1-697b7f4665-zpwzs           0/1       Error                  1          43s       172.16.1.237   tw-node1237

如何删除不需要的分支：  gitlab-repo-branches
setterm -inversescreen on 终端变白
ps 命令虽然在收集运行在系统上的进程信息时非常有用,但也有不足之处:它只能显示某个特定时间点的信息。如果想观察那些频繁换进换出的内存的进程趋势,用 ps 命令就不方便了。
而 top 命令刚好适用这种情况。 top 命令跟 ps 命令相似,能够显示进程信息,但它是实时显示的。

1224
grep

[root@tw-node1237 ~]# history
    1  systemctl status kubelet
    2  systemctl stop kubelet
    3  systemctl status kubelet
    4  systemctl start kubelet
    5  systemctl status kubelet
    6  systemctl status kubelet -l
    7  docker version
    8  docker ps
    9  sysctmctl status docker
   10  systemctl status docker
   11  systemctl restart docker
   12  systemctl status docker.service
   13  systemctl status docker.service -l
   14  ps -efww | grep docker
   15  systemctl status docker.service -l
   16  journalctl -xe
   17  df -h
   18  vi /etc/fstab 
   19  exit
   20   mount /dev/vda /var/lib/docker
   21  df -h
   22  systemctl restart docker.service -l
   23  systemctl status docker.service
   24  kubectl get nodes
   25  kubectl get po
   26  kubectl get po -owide
   27  kubectl logs guardian-apacheds-guardian-847cf7dbbd-42pfh
   28  kubectl get po -owide
   33  blkid /dev/vda
   34  UUID="c2a7c600-3e48-44e5-b3bd-cbac431c3b4e" /var/lib/docker xfs defaults 0 0
   35  vi /etc/fstab
   36  cat /etc/fstab
   37  docker images
   38  docker ps
   39  docker ps |grep inceptor
   40  docker version
   41  docker run -it 844 bash
   42  docker run --help
   43  kubectl get po |grep inceptor
   44  docker images | grep inceptor
   45  docker run it 52f bash
   46  docker run it 968 bash
   47  docker restart
   48  docker run it 52f bash
   49  docker run -it 52f bash
   50  kubectl get po -o wide | grep inceptor
   51  kubectl exec -it inceptor-server-inceptor1-7cdffb54d7-xht9l bash'
   52  kubectl exec -it inceptor-server-inceptor1-7cdffb54d7-xht9l bash
   53  kinit admin
   54  exit
   55  quit
   56  logout
   57  :q
   58  kubectl exec -it inceptor-server-inceptor1-7cdffb54d7-xht9l bash
   59  cat /etc/fstab/
   60  cat /etc/fstab
   61  logout
   62  kubectl get po | grep inceptor
   63  kubectl get po -owide | grep inceptor
   64  kubectl logs -f inceptor-server-inceptor1-697b7f4665-6vc5x
   65  kubectl get po -owide | grep inceptor
   66  kubectl get deploy
   67  kubectl delete deploy inceptor-server-slipstream1
   68  kubectl get deploy
   69  kubectl get po -owide | grep inceptor
   70  kubectl get po -owide |
   71  kubectl get po -owide
   72  docker ps
   73  kubectl get po -owide | grep inceptor
   74  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
   75  jmap
   76  jmap -heap
   77  beeline beeline -u "jdbc:hive2://localhost:10000/default;principal=hive/tw-node1237@TDH"
   78  kinit
   79  kinit admin
   80  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
   81  ls
   82  scp '/home/transwarp/Downloads/guardian-plugins/plugins/inceptor-plugin/target/inceptor-plugin-transwarp-6.2.1.jar' root@172.16.1.237:/root
   83  ls
   84  ls -a
   85  rm inceptor-plugin-transwarp-6.2.1.jar
   86  ls
   87  docker ps
   88  docker ps | grep inceptor
   89  docker ps | grep 968
   90  docker ps | grep bash
   91  docker cp inceptor-plugin-transwarp-6.2.1.jar 844:/root
   92  docker images | grep inceptor
   93  docker commit 844 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
   94  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
   95  kubectl get po | grep inceptor
   96  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
   97  docker ps
   98  kubectl get po | grep inceptor
   99  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x  bash
  100  kubectl get po | grep inceptor
  101  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x  bash
  102  kubectl get po | grep inceptor
  103  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x  bash
  104  ls
  105  rm inceptor-exec-8.0.1.jar
  106  ls
  107  mv inceptor-exec-8.0.1.jar /var
  108  ls
  109  docker ps | grep inceptor
  110  kubectl get po -owide | grep inceptor
  111  docker images |  grep inceptor
  112  pwd
  113  ls /root
  114  ls -al /root
  115  docker images |  grep inceptor
  116  docker run -it 968 bash
  117  ls
  118  docker ps | grep inceptor
  119  docker ps
  120  docker ps | grep 968
  121  docker cp inceptor-exec-8.0.1.jar 157:/root
  122  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  123  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  124  docker images | grep inceptor
  125  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  126  kubectl get po | grep inceptor
  127  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  128  kubectl get po -owide | inceptor
  129  kubectl get po -owide | grep inceptor
  130  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  131  ls
  132  rm -r inceptor-exec-8.0.1.jar
  133  ls
  134  rm -r inceptor-plugin-transwarp-6.2.1.jar
  135  ls
  136  scp '/home/transwarp/Downloads/work/hive-0.12.0-transwarp/src/ql/target/inceptor-exec-8.0.1.jar' root@172.16.1.237:/root
  137  ls
  138  docker ps | grep inceptor
  139  docker ps
  140  docker ps | grep 968
  141  docker cp
  142  docker cp inceptor-exec-8.0.1.jar 157:/root
  143  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  144  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  145  docker images | grep inceptor
  146  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  147  kubectl get po | grep inceptor
  148  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  149  kubectl get po | grep inceptor
  150  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  151  select * from default;
  152  show tables in default;
  153  ls
  154  rm -r inceptor-exec-8.0.1.jar
  155  rm -r inceptor-plugin-transwarp-6.2.1.jar
  156  ls
  157  kubectl get po | grep inceptor
  158  docker ps | grep inceptor
  159  docker ps 
  160  docker ps | grep 968
  161  docker cp inceptor-exec-8.0.1.jar 157:/root
  162  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  163  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  164  docker images | grep inceptor
  165  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  166  kubectl get po -owide | grep inceptor
  167  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  168  ls
  169  rm -r inceptor-plugin-transwarp-6.2.1.jar
  170  rm -r inceptor-exec-8.0.1.jar
  171  ls
  172  logout
  173  ls
  174  docker ps | grep inceptor
  175  docker ps 
  176  docker ps | grep 968
  177  docker cp inceptor-exec-8.0.1.jar 157:/root
  178  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  179  ls
  180  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  181  docker images | grep inceptor
  182  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  183  kubectl get po -owide | grep inceptor
  184  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash
  185  ls
  186  rm -r inceptor-plugin-transwarp-6.2.1.jar
  187  rm -r inceptor-exec-8.0.1.jar
  188  ls
  189  ls -al /root
  190  ls
  191  docker ps | grep inceptor
  192  docker ps 
  193  docker ps | grep 968
  194  docker cp inceptor-exec-8.0.1.jar 157:/root
  195  docker cp inceptor-plugin-transwarp-6.2.1.jar 157:/root
  196  docker commit 157 tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  197  docker images | grep inceptor
  198  docker push tw-node1236:5000/transwarp/inceptor:transwarp-6.2.0-final
  199  kubectl get po -owide | grep inceptor
  200  kubectl exec -it inceptor-server-inceptor1-697b7f4665-6vc5x bash


  docker run -it 163 bash
  509  docker images | grep workflow
  510  docker images
  511  docker pull tw-node1236:5000/transwarp/workflow:transwarp-6.2.0-final
  512  docker pull tw-node1236:5000/transwarp/workflow:studio-1.1.0
  513  docker pull 172.16.1.99/gold/workflow:studio-1.1.0-final-2019-08-13-12-30-31-620b62fde11f90e9638091d4c26cc0287e04ae85
  514  docker images | grep workflow
  515  docker images | grep studio
  516  docker imags
  517  docker images
  518  docker run -ti 6b3c8d4b0a07baec433c26f697257b389a2acc5789e1fa866b02075edcb44f35 bash
  519  docker run -it 172.16.1.99/gold/workflow:studio-1.1.0-final-2019-08-13-12-30-31-620b62fde11f90e9638091d4c26cc0287e04ae85 bash
  520  exit
  521  ls
  522  vim /etc/guardian/conf/guardian-site.xml 
  523  vi /etc/guardian/conf/guardian-site.xml 
  524  exit
  525  df -h
  526  cd /var/log
  527  ls -al
  528  rm messages-201911* -rf
  529  ls -al
  530  df -h
  531  kubectl get po -owide
  532  vi /var/log/hdfs1/hadoop-hdfs-namenode-tw-node1237.log
  533  kubectl get po -owide
  534  vi /var/log/inceptor1/hive-server2.log 
  537  vi /var/log/hdfs1
  538  kubectl get po -owide | grep inceptor
命令列界面 CLI
curl -L -H "PRIVATE-TOKEN: swch1r1jLV-fsMe7ZMJn" "http://172.16.1.41:10080/api/v4/projects/402/jobs/artifacts/guardian-3.1/download?job=postcommit" -o artifact.zip
    unzip artifact.zip
    mkdir -p guardian-server-boot/src/main/resources/static
    cp -r public/* guardian-server-boot/src/main/resources/static/

rm artifact.zip -rf
rm public -rf
rm guardian-server-boot/src/main/resources/static -rf
拉取前端页面

bash shell用一个叫作环境变量(environment variable)的特性来存储有关shell会话和工作环境的信息(这也是它们被称作环境变量的原因)。这项特性允许你在内存中存储数据,以便程序或shell中运行的脚本能够轻松访问到它们。这也是存储持久数据的一种简便方法。

命令 env 、 printenv 和 set 之间的差异很细微。 set 命令会显示出全局变量、局部变量以及用户定义变量。它还会按照字母顺序对结果进行排序。 env 和 printenv 命令同 set 命令的区别在于前两个命令不会对变量排序,也不会输出局部变量和用户定义变量。在种情况下, env 和 printenv 的输出是重复的。不过 env 命令有一个 printenv 没有的功能,这使得它要更有用一些

YAML 是专门用来写配置文件的语言，非常简洁和强大，远比 JSON 格式方便。

echo = 可以通过等号给环境变量赋值,值可以是数值或字符串。注意不要加空格 否则会被识别成单独命令
创建全局环境变量的方法是先创建一个局部环境变量,然后再把它导出到全局环境中。这个过程通过 export 命令来完成,变量名前面不需要加 $ 。
可以用 unset 命令完成删除操作。在 unset 命令中引用环境变量时,记住不要使用 $

https://172.26.2.6:8380/
admin/123
QA集群

list范型赋值用的是适配器模式
Hadoop分布式文件系统（HDFS）允许管理员为所使用的名称数量和单个目录使用的空间量设置配额。name quota和space quota独立运作，但这两种配额的管理和实施是相当类似的。


[root@tw-node1237 ~]# ls
anaconda-ks.cfg                         guardian-client-guardian-3.1.3.jar  guardian-core-guardian-3.1.3.jar         inceptor-exec-8.0.1.jar              lib.tar.gz
fortress-core-1.0.0-guardian-3.1.3.jar  guardian-common-guardian-3.1.3.jar  guardian-server-boot-guardian-3.1.3.jar  inceptor-plugin-transwarp-6.2.1.jar  tos.tar.gz

换包

find / -name 'guardian-client*' 查找文件

find . -name "*.jar"| awk '{print "jar -tf "$1}'| sh -x | grep -i "/GuardianClient.class"
hbase-shaded-server-1.3.1-transwarp-6.2.0.jar

https://172.16.1.237:8380/swagger-ui.html#!/perms/findUserDatasourceUsingGET

1225
存在permobj的objName顺序是Table_or_)view->DB->tb
存在permobjdn的string字段中getdn顺序是 tb->db->Table_or_view

2019-12-25 10:04:06,614 ERROR inceptor.GuardianAuthorizer: (GuardianAuthorizer.java:hasAnyTablePrivilegesOnDB(648)) [HiveServer2-Handler-Pool: Thread-237(SessionHandle=d6315725-863a-4db1-b498-4c5b53868b20)] - Fail to filter tables by privileges. username: [test], dbName:[default]

java.lang.IndexOutOfBoundsException: Index: 2, Size: 2
        at java.util.ArrayList.rangeCheck(ArrayList.java:653)
        at java.util.ArrayList.get(ArrayList.java:429)
        at org.apache.directory.fortress.core.impl.PermDAO.getNextLevelObjName(PermDAO.java:3308)
        at org.apache.directory.fortress.core.impl.PermDAO.findUserAuthorizedDataNodes(PermDAO.java:2483)
        at org.apache.directory.fortress.core.impl.PermP.searchDataNodes(PermP.java:220)
        at org.apache.directory.fortress.core.impl.ReviewMgrImpl.userAuthorizedDataNodes(ReviewMgrImpl.java:497)
        at io.transwarp.guardian.core.manager.PermManager.userAuthorizedDataNodes(PermManager.java:183)
        at

docker image rmi id -f

权限控制表 (ACL: Access Control List)
用来描述权限规则或用户和权限之间关系的数据表。
DAC 自主访问控制
系统会识别用户，然后根据被操作对象的ACL或者权限控制矩阵来控制用户权限
如windows的权限
MAC 强制访问控制
MAC是为了弥补DAC权限控制过于分散的问题而诞生的。在MAC的设计中，每一个对象都都有一些权限标识，每个用户同样也会有一些权限标识，而用户能否对该对象进行操作取决于双方的权限标识的关系，这个限制判断通常是由系统硬性限制的。比如在影视作品中我们经常能看到特工在查询机密文件时，屏幕提示需要“无法访问，需要一级安全许可”，这个例子中，文件上就有“一级安全许可”的权限标识，而用户并不具有。
RBAC 基于角色的访问控制
RBAC在用户和权限之间引入了“角色（Role）”的概念（暂时忽略Session这个概念）
简单来说RBAC就是：用户关联角色，角色关联权限。另外，RBAC是可以模拟出DAC和MAC的效果的。
ABAC 基于属性的权限验证
不同于常见的将用户通过某种方式关联到权限的方式，ABAC则是通过动态计算一个或一组属性来是否满足某种条件来进行授权判断（可以编写简单的逻辑）。属性通常来说分为四类：用户属性（如用户年龄），环境属性（如当前时间），操作属性（如读取）和对象属性（如一篇文章，又称资源属性），所以理论上能够实现非常灵活的权限控制，几乎能满足所有类型的需求。

1226
https://magnetw.app/

如上文所述，ThreadLocal 适用于如下两种场景
http://www.jasongj.com/java/threadlocal/
每个线程需要有自己单独的实例
实例需要在多个方法中共享，但不希望被多线程共享

    SourceIp：访问服务的用户的源地址ip
    UserName：访问服务的用户的用户名
    GroupName： 访问服务的用户所属的组
    RoleName： 访问服务的用户所属的角色
    CurrentTime： 访问服务的时间
    Resource： 访问服务的资源名字，例如对于inceptor表，表示方法为default.alice_tbl；对于hdfs目录则为 /user/alice等

局限：目前支持ABAC的插件只有Incpetor的插件，其他服务的插件正在开发中

找出v1中使用abac的部分并对应找出相对v2的部分
已知abac目前只对inceptor有用 对应resource的部分
表 gs_abac_policies gs_policy_statement gs_resource_services

http://172.16.1.41:10080/guardian/guardian/blob/master/docs/ABAC.md

ABAC模型由如下几个部分组成：

PEP(Policy Enforcement Point): 负责使用ABAC策略保护用户数据以及应用。PEP处理访问请求，获得环境上下文，并将这些信息发送给PDP。
PEP的功能是由Guardian Plugin实现的。
PDP(Policy Decision Point): 是ABAC的处理单元，根据所有的策略和PEP发送的请求来决定是Allow还是Deny，PDP使用PIP获得策略和
属性等信息。PDP单元应该是一个可以复用的模块，可以放置到Guardian Server中通过API判断权限；并且需要可以单独提取出来，放到Guardian Plugin
中，在宿主服务中直接判断权限，并借以PIP创建的缓存以及PDP自身的缓存，最大限度的提升权限判断的性能。
PIP(Policy Information Point): 搭建PDP到外部数据源（LDAP数据目录或者关系型数据库）的桥梁。
PAP(Policy Administration Point): 策略管理终端，Guardian Server扮演了PAP的角色。

PEP->PDP->PIP->PAP
 guardianAbacEnabled = this.guardianConf.getBoolean(GuardianVars.GUARDIAN_ABAC_AUTHORIZATION_ENABLED.varname,
            GuardianVars.GUARDIAN_ABAC_AUTHORIZATION_ENABLED.defaultBoolVal);

    if (guardianAbacEnabled) {
      try {
        policyEngine = PolicyEngineFactory.getInstance(this.guardianConf);
        policyEngine.start();
      } catch (GuardianClientException e) {
        LOG.error("Fail to initialize Guardian policy engine when initializing Guardian AccessController", e);
        // TODO: deal with ErrorMsg
        throw new HiveAuthzPluginException(e,
                "Fail to initialize Guardian policy engine when initializing Guardian AccessController", ErrorMsg.GENERIC_ERROR);
      }

v1
PEP: checkPermission->checkPolicy->
PDP: policyEngine.checkPolicy->PolicyUtil.checkStatement->PolicyUtil.checkPolicy
getRelatedStatement Exprs
PolicyUtil.checkStatements(expr.evaluate(cloneContext))->PolicyUtil.evaluate->StatementExpression.evaluate->exp/operator
->guardianClient.getstatements->PolicyController.getStatements->PolicyManager.->policyDao.getStatements->PolicyDaoImpl.getStatements->mapper.getStatements
gs_policy_statement -> statement_id policy_id json
一个策略可以是允许(ALLOW)类型或者拒绝(DENY)类型的策略，允许类型的策略表示，当上下文中的标签满足满足条件的时候允许用户访问资源，而拒绝策略则相反，当上下文中的标签满足条件的时候，则拒绝访问资源。注意，DENY的优先级高于ALLOW，也就是如果一组策略都满足，那么存在任意一个DENY策略，访问都会被拒绝。点击右上角的“拒绝策略”按钮进行切换。默认情况下是ALLOW类型策略，点击之后左上角会出现一个红色的拒绝图标，表示改策略为一个DENY类型策略。再次点击则切换回ALLOW策略。

PIP: PolicyController->PolicyManager->policyDao->PolicyDaoImpl->PolicyMapper
PAP: 终端

v2
PolicyEngine.checkPolicy->getRelatedStatementExprs/PolicyUtils.checkStatements->
StateExpressions.evaluate->Expression.evalate/opertor

gs_statement_resources
gs_resources
gs_abac_policies
gs_policy_statement

etc/conf/guardian/guardian-site加数据库密码


controller.userpermission->findPrincPermissions->permManager.getPrincPerms->permDao.searchPrincPermsOnService->permDaoImpl.searchPrincPermsOnService->permMapper.selectServiceUserPerms->
1. abac guadian版本适配问题 
2. v2权限需要加action过滤 1
3. abac的resource资源变动是否影响
选择所有与此次权限判断的用户、资源、上下文相关的policy, 对策略权限进行判断。ResourceExpr继承于StringExpr，contail模式
获取到json模式转成resourcevo做匹配
每次启动根据请求把对应policy的statement拉下来，并形成关系树。查找和resource相关的
Node used to record a resource node and related policies

192.168.76.128：22

minidwep

1227

findPrincPermissions(SessionVo sessionVo, PrincipalVo principalVo,
                                                  String component, List<String> dataSource,
                                                  String substring, String action, boolean subtree, Boolean inheritance)
selectServiceUserPerms0
selectServiceUserPermsByRole0
selectServiceRolePerms0
selectServiceGroupPerms0
selectServicePermsByGroups0
selectServicePermsByGroupsRoles0


selectUserResourcePerms0
selectRoleResourcePermsByUser0
selectRoleResourcePerms0
selectGroupResourcePerms0
selectGroupsResourcePerms
selectRolesResourcePerms0

selectUserPerms
selectRolePermsByUser
selectGroupsPerms
selectRolesPerms

        <if test="searchValue != null">
            AND MATCH(R.path) AGAINST (#{searchValue})
        </if>

selectGroupAuthorizedResources
selectRoleAuthorizedResources
selectRoleAuthorizedResourcesByUser
selectUserAuthorizedResources
selectGroupsAuthorizedResources
selectRolesAuthorizedResources

getAuthorizedResources

开启abac时
用户执行操作 先检查是否存在关联操作关系的策略，
server会抽取所有与此次权限判断的用户、资源、上下文相关的policy，判断在抽取到策略中是否存在显式的Deny。具体会把所有的statement拉下来，形成一颗关系树。根据操作涉及的resource提取相关的策略exp进行判断是否存在deny。这个策略中存在显式deny的话，拒绝；存在deny_depends并且不存在allow的时候拒绝。存在allow的时候允许；存在not_apply的时候not_apply,默认结果为deny。
用户先拿到操作的resource和action，然后checkpolicy，传的参数是user，resource，action和关于ip和时间的context
server端开启一端缓存，并拉下对应操作组件的所有policy的node，形成一颗关系树
Node used to record a resource node and related policies
找到和要操作的resource相对应的statementExprs
具体做法：1.先找到特定资源的statement，并拉和它有关系的子节点
2.找global statement
3.找孤儿statement
返回之后判断是否通过evaluate
由于每次开启abac都会拉statement判断，不涉及其他字段对应v1和v2的改动
getRelatedStatementExprs

1.fetch statement时会不会有冲突->来源未知 swagger传serviceid 应该不包括字段冲突的情况
2.和resource验证时会不会有冲突

文件系统->为硬盘中存储的0和1的应用和使用文件搭建起了桥梁
ext文件系统->扩展文件系统，使用虚拟目录操作硬件设备，在物理设备上按定长的块存储数据
linux通过唯一的数值(称作索引节点号)来引用索引节点表中的每个索引节点,这个值是创建文件时由文件系统分配的。文件系统通过索引节点号而不是文件全名及路径来标识文件。
ext2文件系统->1.创建时间值等辅助系统追踪文件访问情况/允许最大文件增加到2tb/按组分配磁盘块来减轻内存碎片化
日志文件系统->先将文件更改写入临时文件，当数据写到存储设备和索引节点表后，再删除对应的日志条目
ext3文件系统->索引节点表+日志文件
ext4文件系统->压缩加密+区段（区段在存储设备上按块分配空间,但在索引节点表中只保存起始块的位置）+块预分配
Reiser->只支持日志回写模式，把索引节点表写到日志文件
JFS->JFS文件系统采用的是有序日志方法,即只在日志中保存索引节点表数据,直到真正的文件
数据被写进存储设备时才删除它。
,日志式的另一种选择是一种叫作写时复制 (copy-on-write,COW)的技术。
COW利用快照兼顾了安全性和性能。如果要修改数据,会使用克隆或可写快照。修改过的数据
并不会直接覆盖当前数据,而是被放入文件系统中的另一个位置上。即便是数据修改已经完成,
之前的旧数据也不会被重写。
COW文件系统已日渐流行,接下来会简要概览其中最流行的两种(Btrf和ZFS)。

检查table的owner不会很奇怪嘛 一个用户有创建表的权限 没有表内权限我觉得是设计错误不是逻辑错误直接sayno就好，改后会加一个tbls结果差集的循环时间，而且这么细化以后每个object都要查一下owner？
showdatabase看不到的库不能操作

1230

完成：
1. 完成show权限的镜像打包和测试，修改client和hive工程相应部分并提交

db有权限->tb一定有权限?
tbowner一定要循环获取？

2. 检查ABAC策略对v2版本资源适配的迁移状况

开启abac时
用户执行操作 先检查是否存在关联操作关系的策略，
server会抽取所有与此次权限判断的用户、资源、上下文相关的policy，判断在抽取到策略中是否存在显式的Deny。具体会把所有的statement拉下来，形成一颗关系树。根据操作涉及的resource提取相关的策略exp进行判断是否存在deny。这个策略中存在显式deny的话，拒绝；存在deny_depends并且不存在allow的时候拒绝。存在allow的时候允许；存在not_apply的时候not_apply,默认结果为deny。
用户先拿到操作的resource和action，然后checkpolicy，传的参数是user，resource，action和关于ip和时间的context
server端开启一端缓存，并拉下对应操作组件的所有policy的node，形成一颗关系树
Node used to record a resource node and related policies
找到和要操作的resource相对应的statementExprs
具体做法：1.先找到特定资源的statement，并拉和它有关系的子节点
2.找global statement
3.找孤儿statement
返回之后判断是否通过evaluate
由于每次开启abac都会拉statement判断，不涉及其他字段对应v1和v2的改动
resourceId

3. 添加关于v2版本show权限需要加action条件过滤的接口改动部分

进行中：
1. search组件在v2版本的迁移测试

本周：
1. 维护已提交代码并对v2重构做相应改动工作

705-626
 List<String> filterDatabaseByPrivileges(String username, List<String> dbs) {
    try {
      if (!guardianConf.getBoolean(GuardianVars.GUARDIAN_INCEPTOR_FILTER_SHOWTABLES.varname, GuardianVars.GUARDIAN_INCEPTOR_FILTER_SHOWTABLES.defaultBoolVal)) {
        return dbs;
      }


      List<String> dbPerms = guardianClient.userAuthorizedDataNodes(username, component, Arrays.asList("TABLE_OR_VIEW"), null, null);
      Set<String> dbsWithPrivsSet = new HashSet<>();
      if (dbPerms != null) {
        for (String db : dbPerms) {
          dbsWithPrivsSet.add(db.toLowerCase());
        }
      }
      for (String db : dbs) {
        try {
          boolean isDBOwner = SQLAuthorizationUtils.isOwner(username, Collections.EMPTY_LIST,
              new HivePrivilegeObject(HivePrivilegeObject.HivePrivilegeObjectType.DATABASE, db, null));
          if (isDBOwner) {
            dbsWithPrivsSet.add(db.toLowerCase());
          }
        } catch (HiveAuthzPluginException ex) {
          LOG.error("Fail to check if user [{}] is the owner of database [{}].", username, db, ex);
        }
      }
      List<String> dbsWithPrivs = new ArrayList<>(dbsWithPrivsSet);
      dbs.retainAll(dbsWithPrivs);
      return dbs;
    } catch (GuardianClientException e) {
      LOG.error("Fail to filter DBs by privileges. username: [{}]", username, e);
      return dbs;
    }
  }

  boolean hasAnyGlobalTablePrivileges(String username) {
    List<String> dataSource = InceptorPermUtil.global();
    List<PermissionVo> permVos = new ArrayList<>();
    permVos.add(new PermissionVo(component, dataSource, ADMIN_PERM));
    for (GuardianSQLPrivilegeType privilegeType : GuardianSQLPrivilegeType.ALL_ON_TABLE) {
      permVos.add(new PermissionVo(component, dataSource, privilegeType.name()));
    }
    try {
      return guardianClient.checkAnyAccess(username, permVos);
    } catch (GuardianClientException e) {
      LOG.error("Fail to check if user [{}] has any global table privileges.", username, e);
      return true;
    }
  }

  List<String> filterTablesByPrivileges(String username, String dbName, List<String> tbls) {
    try {
      boolean isDBOwner = SQLAuthorizationUtils.isOwner(username, Collections.EMPTY_LIST,
          new HivePrivilegeObject(HivePrivilegeObject.HivePrivilegeObjectType.DATABASE, dbName, null));
      if (isDBOwner) {
        return tbls;
      }
    } catch (HiveAuthzPluginException ex) {
      LOG.error("Fail to check if user [{}] is the owner of database [{}].", username, dbName, ex);
    }
    try {
      List<String> dbPerms = guardianClient.userAuthorizedDataNodes(username, component, Arrays.asList("TABLE_OR_VIEW"), null, null);
      if (dbPerms.contains(dbName)) {
        return tbls;
      }
      List<String> tbPerms = guardianClient.userAuthorizedDataNodes(username, component, InceptorPermUtil.convertDatabase(dbName), null, null);
      Set<String> tblsWithPrivsSet = new HashSet<>();
      if (tbPerms != null) {
        for (String tb : tbPerms) {
          tblsWithPrivsSet.add(tb.toLowerCase());
        }
      }
      List<String> tblsWithPrivs = new ArrayList<>(tblsWithPrivsSet);
      tbls.retainAll(tblsWithPrivs);
      return tbls;
    } catch (GuardianClientException e) {
      LOG.error("Fail to filter tables by privileges. username: [{}], dbName:[{}]", username, dbName, e);
      return tbls;
    }
  }

EnumSet<GuardianSQLPrivilegeType>->自己写的
mvn缓存不起作用
io.transwarp.guardian.plugins.inceptor.GuardianHiveAuthorizer

hive那边版本号是变掉了吗？你有没有rebase过啥的
idea local history

  <parent>
    <groupId>org.apache.hive</groupId>
    <artifactId>hive</artifactId>
    <version>8.0.2</version>
    <relativePath>../pom.xml</relativePath>
  </parent>
 <hive.version>8.0.2</hive.version>
6.2.2

blic static void main(String[] args) {
    List<String> list1 = new ArrayList<String>();
    list1.add("A");
    list1.add("B");
    list1.add("C");

    List<String> list2 = new ArrayList<String>();
    list2.add("C");
    list2.add("B");
    list2.add("D");
    // 并集
    list1.addAll(list2);
    // 去重复并集
    list2.removeAll(list1);
    list1.addAll(list2);
    // 交集
    list1.retainAll(list2);
    // 差集
    list1.removeAll(list2);
}

1231
du -h --max-depth=1
CollectionsUtil.isEmpty()
service transwarp-manager restart 服务重启
search搜索资源通过keymapper

[sudo] password for transwarp: 
E: Could not get lock /var/lib/dpkg/lock - open (11: Resource temporarily unavailable)
E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it?

在终端中敲入以下两句

sudo rm /var/cache/apt/archives/lock

sudo rm /var/lib/dpkg/lock

如何编写脚本：
touch hello.sh
chmod +x hello.sh
vim hello.sh
./hello.sh
vim 文件------>进入文件----->命令模式------>按i进入编辑模式----->编辑文件 ------->按Esc进入底行模式----->输入:wq/q! （输入wq代表写入内容并退出，即保存；输入q!代表强制退出不保存。）)

在idea如何快速重载一个方法
ctrl+O找到方法双击

throw new LdapOperationException(ResultCodeEnum.UNWILLING_TO_PERFORM, "Write on slave node is forbidden!");

Cannot instantiate interface org.springframework.context.ApplicationListener : io.transwarp.guardian.server.boot.applistener.AppPreparedEventListener
mvn clean
ldapadd 

ldap指令
1）增加增：ldapadd
选项：
-x   进行简单认证
-D   用来绑定服务器的DN
-h   目录服务的地址
-w   绑定DN的密码
-f   使用ldif文件进行条目添加的文件
 
例子：
将 test.ldif 中的数据导入 ldap
[root@openldap ~]# ldapadd -x -D "cn=root,dc=kevin,dc=com" -w secret -f /root/test.ldif
 
2）删除：ldapdelete
例子
[root@openldap ~]# ldapdelete -x -D "cn=userACI,ou=People,${guardian_ds_domain}" -w secret "ou=People,${guardian_ds_domain}"
[root@openldap ~]# ldapdelete -x -D 'cn=root,dc=it,dc=com' -w secert 'uid=zyx,dc=it,dc=com'
 
3）修改：ldapmodify
选项
-a 添加新的条目.缺省的是修改存在的条目.
-C 自动追踪引用.
-c 出错后继续执行程序并不中止.缺省情况下出错的立即停止.
-D binddn 指定搜索的用户名(一般为一dn 值).
-e 设置客户端证书文件,例: -e cert/client.crt
-E 设置客户端证书私钥文件,例: -E cert/client.key
-f file 从文件内读取条目的修改信息而不是从标准输入读取.
-H ldapuri 指定连接到服务器uri。常见格式为ldap://hostname:port
-h ldaphost 指定要连接的主机的名称/ip 地址.它和-p 一起使用.
-p ldapport 指定要连接目录服务器的端口号.它和-h 一起使用.
-M[M] 打开manage DSA IT 控制. -MM 把该控制设置为重要的.
-n 用于调试到服务器的通讯.但并不实际执行搜索.服务器关闭时,返回错误；服务器打开时,常和-v 参数一起测试到服务器是否是一条通路.
-v 运行在详细模块.在标准输出中打出一些比较详细的信息.比如:连接到服务器的ip 地址和端口号等.
-V 启用证书认证功能,目录服务器使用客户端证书进行身份验证,必须与-ZZ 强制启用TLS 方式配合使用,并且匿名绑定到目录服务器.
-W 指定了该参数,系统将弹出一提示入用户的密码.它和-w 参数相对使用.
-w bindpasswd 直接指定用户的密码. 它和-W 参数相对使用.
-x 使用简单认证.
-Z[Z] 使用StartTLS 扩展操作.如果使用-ZZ,命令强制使用StartTLS 握手成功.
 
例子
[root@openldap ~]# ldapmodify -x -D "cn=root,dc=it,dc=com" -W -f modify.ldif

transwarp@transwarp-Latitude-5480:~/Downloads/work/guardian-backend/guardian$ slappasswd
New password: 
Re-enter new password: 
{SSHA}0zQyKzgkwvpSt03maoymG1v1ey3/D7uD

dn: olcDatabase={0}config,cn=config
changetype: modify
add: olcRootPW
olcRootPW: {SSHA}0zQyKzgkwvpSt03maoymG1v1ey3/D7uD 
EOF

1: dn:dc=dlw,dc=com
2: objectclass:top
3: objectclass:dcobject
4: objectclass:organization
5: dc:dlw
6: o:dlw,Inc.
7:
8: dn:ou=managers, dc=dlw, dc=com
9: ou:managers
10: objectclass:organizationalUnit
11:
12: dn:cn=dlw,ou=managers,dc=dlw,dc=com
13: cn:dlw
14: sn:wangshibo
15: objectclass:person
16:
17: dn:cn=test,ou=managers,dc=dlw,dc=com
18: cn:test
19: sn:Test User
20: objectclass:person

ldapadd -x -D "ou=People1,${guardian_ds_domain}" -w secret -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/config.ldif

2019-12-31 16:58:09,612 INFO org.apache.directory.api.ldap.model.entry.AbstractValue: ERR_04447_CANNOT_NORMALIZE_VALUE Cannot normalize the wrapped value ERR_04226 I do not know how to handle NameAndOptionalUID normalization with objects of class: nsAIMid
2019-12-31 16:58:09,731 INFO org.apache.directory.server.core.api.CacheService: fetching the cache named system-entryDn

2019-12-31 17:03:19,242 WARN io.transwarp.guardian.apacheds.synchronization.ApacheDsSyncManager: Delete users under ou=People,dc=tdh encountered exception
org.apache.directory.api.ldap.model.exception.LdapOperationException: Delete operation on Apacheds is forbidden!

2019-12-31 17:03:19,309 ERROR io.transwarp.guardian.apacheds.synchronization.ApacheDsSyncManager: Failed to add group hbase.
org.apache.directory.api.ldap.model.exception.LdapOperationException: Add operation on Apacheds is forbidden!

ou=People1,${guardian_ds_domain}

ldapadd -x -D "uid=admin,ou=system" -w Transwarp! -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/config.ldif
 
ldapadd -x -D "cn=userAC,ou=People,dc=tdh}" -w Warp1234 -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/initPartition.ldif


/home/transwarp/Downloads/work/guardian-backend/guardian/chrootpw.ldif

ldapdelete -H ldap://localhost:10389 -D "cn=userACI,ou=People,dc=tdh" -w Warp1234 < /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/initPartition.ldif


cn=userACI,ou=People,dc=tdh

ldapadd -x -D "cn=test,ou=Config,dc=tdh"  -W -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/initPartition.ldif

ldappasswd -x -D "cn=test,ou=Config,dc=tdh" -W "cn=test,ou=Config,dc=tdh" -S

ldapsearch -x -b "cn=test,ou=Config,dc=tdh"

ldapdelete -D "cn=root,ou=People,dc=tdh" -W -x "cn=userACI,ou=People,dc=tdh"Transwarp!

这句话
ldapadd -x -D "uid=admin,ou=system" -w Transwarp! -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/initPartition.ldif

1.测试的密码是Transwarp!
改动前测一下和现在的一不一样
transwarp@transwarp-Latitude-5480:~/Downloads/work/guardian-backend/guardian$ ldapadd -x -D "uid=admin,ou=system" -w Transwarp! -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/config.ldif
ldap_bind: Invalid credentials (49)
transwarp@transwarp-Latitude-5480:~/Downloads/work/guardian-backend/guardian$ ldapadd -x -D "uid=admin,ou=system" -w Transwarp! -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/config.ldif
ldap_bind: Invalid credentials (49)

2. 238行   operationContext CoreSession getSession();
bind-abstract-operation
能不能把adminsession提出来 看是不是内部的操作 然后在add操作时过滤掉


1.2

安装ldaputil
 1730  ldapsearch -H ldap://localhost:10389 -D "uid=admin,ou=system" -W -b "ou=people,dc=tdh"

ldapsearch --help

Search options:
  -a deref   one of never (default), always, search, or find
  -A         retrieve attribute names only (no values)
  -b basedn  base dn for search
  -c         continuous operation mode (do not stop on errors)
  -E [!]<ext>[=<extparam>] search extensions (! indicates criticality)
             [!]domainScope              (domain scope)
             !dontUseCopy                (Don't Use Copy)
             [!]mv=<filter>              (RFC 3876 matched values filter)
             [!]pr=<size>[/prompt|noprompt] (RFC 2696 paged results/prompt)
             [!]sss=[-]<attr[:OID]>[/[-]<attr[:OID]>...]
                                         (RFC 2891 server side sorting)
             [!]subentries[=true|false]  (RFC 3672 subentries)
             [!]sync=ro[/<cookie>]       (RFC 4533 LDAP Sync refreshOnly)
                     rp[/<cookie>][/<slimit>] (refreshAndPersist)
             [!]vlv=<before>/<after>(/<offset>/<count>|:<value>)
                                         (ldapv3-vlv-09 virtual list views)
             [!]deref=derefAttr:attr[,...][;derefAttr:attr[,...][;...]]
             [!]<oid>[=:<b64value>] (generic control; no response handling)
  -f file    read operations from `file'
  -F prefix  URL prefix for files (default: file:///tmp/)
  -l limit   time limit (in seconds, or "none" or "max") for search
  -L         print responses in LDIFv1 format
  -LL        print responses in LDIF format without comments
  -LLL       print responses in LDIF format without comments
             and version
  -M         enable Manage DSA IT control (-MM to make critical)
  -P version protocol version (default: 3)
  -s scope   one of base, one, sub or children (search scope)
  -S attr    sort the results by attribute `attr'
  -t         write binary values to files in temporary directory
  -tt        write all values to files in temporary directory
  -T path    write files to directory specified by path (default: /tmp)
  -u         include User Friendly entry names in the output
  -z limit   size limit (in entries, or "none" or "max") for search
Common options:
  -d level   set LDAP debugging level to `level'
  -D binddn  bind DN
  -e [!]<ext>[=<extparam>] general extensions (! indicates criticality)
             [!]assert=<filter>     (RFC 4528; a RFC 4515 Filter string)
             [!]authzid=<authzid>   (RFC 4370; "dn:<dn>" or "u:<user>")
             [!]chaining[=<resolveBehavior>[/<continuationBehavior>]]
                     one of "chainingPreferred", "chainingRequired",
                     "referralsPreferred", "referralsRequired"
             [!]manageDSAit         (RFC 3296)
             [!]noop
             ppolicy
             [!]postread[=<attrs>]  (RFC 4527; comma-separated attr list)
             [!]preread[=<attrs>]   (RFC 4527; comma-separated attr list)
             [!]relax
             [!]sessiontracking
             abandon, cancel, ignore (SIGINT sends abandon/cancel,
             or ignores response; if critical, doesn't wait for SIGINT.
             not really controls)
  -h host    LDAP server
  -H URI     LDAP Uniform Resource Identifier(s)
  -I         use SASL Interactive mode
  -n         show what would be done but don't actually do it
  -N         do not use reverse DNS to canonicalize SASL host name
  -O props   SASL security properties
  -o <opt>[=<optparam>] general options
             nettimeout=<timeout> (in seconds, or "none" or "max")
             ldif-wrap=<width> (in columns, or "no" for no wrapping)
  -p port    port on LDAP server
  -Q         use SASL Quiet mode
  -R realm   SASL realm
  -U authcid SASL authentication identity
  -v         run in verbose mode (diagnostics to standard output)
  -V         print version info (-VV only)
  -w passwd  bind password (for simple authentication)
  -W         prompt for bind password
  -x         Simple authentication
  -X authzid SASL authorization identity ("dn:<dn>" or "u:<user>")
  -y file    Read password from file
  -Y mech    SASL mechanism
  -Z         Start TLS request (-ZZ to require successful response)

ldapsearch -H ldap://localhost:10389 -D "uid=admin,ou=system" -W -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/initPartition.ldif


transwarp@transwarp-Latitude-5480:~$ ldapadd -help
ldap_sasl_interactive_bind_s: Can't contact LDAP server (-1)
ldap_bind: Invalid credentials (49)

Exception in thread "main" org.apache.directory.api.ldap.model.exception.LdapOperationException: Modify operation on Apacheds is forbidden!
        at io.transwarp.guardian.apacheds.synchronization.GuardianAuthenticationInterceptor.modify(GuardianAuthenticationInterceptor.java:178)
        at org.apache.directory.server.core.api.interceptor.BaseInterceptor.next(BaseInterceptor.java:506)
        at org.apache.directory.server.core.normalization.NormalizationInterceptor.modify(NormalizationInterceptor.java:216)

git rm --cached <FILE>：已 add（tracked） 未 commit 的文件，使其回到未 add 状态（untracked）。
git rm -f <FILE>： 从本地删除已 add 的文件。
git checkout -- <FILE> ：有修改的文件回到 tracked 状态，对已 tracked 的文件撤销修改。
git reset HEAD <FILE>：撤销 commit，回到 modified 状态。
git reset --soft HEAD^：撤销 commit，回到 tracked 状态。
git clean：删除所有 untracked 文件。
好基友：git reset --hard && git clean -f 使本地完全回退到上次 commit
kubectl cp kadmin.guardian guardian-server-guardian-549ff9455b-qf8kq:/root
ifconfig查ip->用wifi的那个
chmod 777 file ->所有用户可读可写可执行  chmod 754 创建者/群组/其他用户
[root@tw-node1237 ~]# ./kadmin.guardian -H 172.16.202.145 -w Transwarp! -r TDH -q "addprinc -pw 123456 simple/tw-node593"
refactor


transwarp@transwarp-Latitude-5480:~/Downloads$ echo http_proxy
http_proxy
transwarp@transwarp-Latitude-5480:~/Downloads$ echo $http_proxy
http://172.16.0.249:3128/

进入数据库 mysql -uroot -proot

ldapsearch ldapadd
ldapsearch -H ldap://localhost:10389 -D "uid=admin,ou=system" -W -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/initPartition.ldif
ldapadd -x -D "uid=admin,ou=system" -w Transwarp! -f /home/transwarp/Downloads/work/guardian-backend/guardian/apacheds-backend/src/main/resources/ldif/config.ldif

切换tag
git remote update --prune 
git checkout -b guardian-3.1.2-final trunk/guardian-3.1.2-final



邮件
hive.version 8.0.1 tag:transwarp-6.2.1-final
guardian.version: guardian-3.1.2  tag:guardian-3.1.2-final
tdh.version: transwarp-6.2.1
examples/target jar
替换jar包 
inceptor镜像里/usr/lib/inceptor/lib 
guardian-client-guardian-xxx.jar -> guardian-client-guardian-3.1.2.jar
guardian-common-guardian-xxx.jar -> guardian-common-guardian-3.1.2.jar
inceptor-exec-xxx.jar -> inceptor-exec-8.0.1.jar
inceptor-plugin-transwarp-xxx.jar -> inceptor-plugin-transwarp-6.2.1.jar

仍然
inceptor镜像
find / -name 'guardian-client*' 查找文件
cd
find . -name "*.jar"| awk '{print "jar -tf "$1}'| sh -x | grep -i "/GuardianClient.class"

删掉
/usr/lib/hbase/lib/guardian-client-guardian-3.1.0.jar
/usr/lib/hadoop-yarn/lib/guardian-client-guardian-3.1.0.jar
/usr/lib/hadoop-mapreduce/guardian-client-guardian-3.1.0.jar
/usr/lib/hadoop/lib/guardian-client-guardian-3.1.0.jar
/usr/lib/hadoop-hdfs/lib/guardian-client-guardian-3.1.0.jar
替换
+ jar -tf ./inceptor/lib/hbase-shaded-server-1.3.1-transwarp-6.2.0.jar


        <tdh.version>transwarp-6.1.0</tdh.version>
        <hadoop.version>2.7.2-${tdh.version}</hadoop.version>
        <hive.version>8.0.1</hive.version>

guardian server镜像中
/usr/lib/guardian/lib
fortress-core-1.0.0-guardian-3.1.3.jar -> fortress-core-1.0.0-guardian-3.1.3.jar
guardian-client-guardian-3.1.3.jar -> guardian-client-guardian-3.1.3.jar
guardian-common-guardian-3.1.3.jar -> guardian-common-guardian-3.1.3.jar
guardian-core-guardian-3.1.3.jar -> guardian-core-guardian-3.1.3.jar
/usr/lib/guardian
guardian-server-boot-guardian-3.1.3.jar -> guardian-server-boot-guardian-3.1.3.jar

拉取前端页面
rm artifact.zip -rf
rm public -rf
rm guardian-server-boot/src/main/resources/static -rf

curl -L -H "PRIVATE-TOKEN: swch1r1jLV-fsMe7ZMJn" "http://172.16.1.41:10080/api/v4/projects/402/jobs/artifacts/guardian-3.1/download?job=postcommit" -o artifact.zip
    unzip artifact.zip
    mkdir -p guardian-server-boot/src/main/resources/static
    cp -r public/* guardian-server-boot/src/main/resources/static/





fortress-core是fortress项目的
guardian-client guardian-common, guardian-core guardian-server-boot是guardianserver项目的
inceptor-exec 是hive项目的
inceptor-plugin-transwarp 是plugin项目的

1.3

[root@tw-node1236 ~]# kubectl get po -owide | grep inceptor
the server doesn't have a resource type "po" -》磁盘满了

df -sh
du -sh *
查找最大文件
 docker system prune 清除镜像缓存
lsof？

du

显示每个文件和目录的磁盘使用空间

命令参数

-c或--total  除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和。

-s或--summarize  仅显示总计，只列出最后加总的值。

-h或--human-readable  以K，M，G为单位，提高信息的可读性。

 
df

显示指定磁盘文件的可用空间

-a 全部文件系统列表

-h 方便阅读方式显示

-i 显示inode信息

-T 文件系统类型

find . -name 'hbase-*'
当前目录下查找

shaded直接删掉也不会有影响？->还是没删

172.26.2.6 root/123456
node206上面没有guardian，所以没有镜像，去node530上面
cat /etc/hosts

docker commit be6 node206:5000/transwarp/guardian:guardian-3.1.2-final
docker commit e24 node206:5000/transwarp/inceptor:transwarp-6.2.1-final

addfront是脚本
./addfront
git rm --cached delfront 
set changelist active

脚本运行和代码运行区别

federartion初始化没好

beeline -u "jdbc:hive2://localhost:10000/default;principal=hive/suse01@TDH"


id int primary key auto_increment,  
	name varchar(50) unique,          
	gender char(1),	                      
	birthday date,		              
	score double	

tar：tar是*nix下的打包工具，生成的包通常也用tar作为扩展名，其实tar只是负责打包，不一定有压缩，事实上可以压缩，也可以不压缩，通常你看到xxxx.tar.gz，就表示这个tar包是压缩的，并且使用的压缩算法是GNU ZIP，而xxxx.tar.bz2就表示这个包使用了bzip2算法进行压缩，当然这样的命名只是一种惯例，并非强制。简单地说，tar就仅是打包。

jar：即Java Archive，Java的包，Java编译好之后生成class文件，但如果直接发布这些class文件的话会很不方便，所以就把许多的class文件打包成一个jar，jar中除了class文件还可以包括一些资源和配置文件，通常一个jar包就是一个java程序或者一个java库。


 war：Web application Archive，与jar基本相同，但它通常表示这是一个Java的Web应用程序的包，tomcat这种Servlet容器会认出war包并自动部署	               

0106
完成：
1. WARP-31985: 增加show tables指令中对DBOwner和TBOwner的过滤，增加在所在库权限的过滤。项目打包测试编写文档 wiki：http://172.16.1.168:8090/pages/viewpage.action?pageId=23471650

2. WARP-40786: 完成apacheds对外表现只读的重构
用kadmin add测试了一下

进行中：
1. search组件在v2版本的迁移测试
G
本周：
1. 维护已提交代码并对v2重构做相应改动工作

  991  mvn clean install -DskipTests
  992  git rm --cached addfront 
  993  git rm --cached delfront 
  994  chmod 777 addfront 
  995  chmod 777 del
  996  chmod 777 delfront 
  997  ./addfront 
  998  ./delfront 
  999  ./addfront 
 1000  mvn clean install -DskipTests


sed文件流编辑器
sed 's/test/trial/g' data4.txt
gawk处理文件
Ctrl+D组合键会在bash中产生一个EOF字符。这个组合键能够终止该gawk程序并返回到命令行界面提示符下。

history N 显示最近的N条命令，例如history 5 
history -d N 删除第N条命令，这个N就是前面的编号，例如history -d 990 
history -c 清空命令历史 
history -a 将当前会话中的命令历史写入指定文件 
echo $HISTFILE 使用此命令查看环境变量

tar和jar
:(){:|:&};

cat /var/logs
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
172.26.2.6 node206
172.26.5.30 node530
172.26.5.31 suse01


先把这些都卸了，最后再把guardian卸了

1.找欣宇对接问题
2.hyperbase插件 region每个regionserver 都会对guardian发送同步心跳


transwarp@transwarp-Latitude-5480:~/Downloads$ mkdir -p  /etc/docker/certs.d/172.16.1.99
mkdir: cannot create directory ‘/etc/docker/certs.d’: Permission denied
transwarp@transwarp-Latitude-5480:~/Downloads$ sudo mkdir -p  /etc/docker/certs.d/172.16.1.99
[sudo] password for transwarp: 
transwarp@transwarp-Latitude-5480:~/Downloads$  cp -r ca.crt /etc/docker/certs.d/172.16.1.99/
cp: cannot create regular file '/etc/docker/certs.d/172.16.1.99/ca.crt': Permission denied
transwarp@transwarp-Latitude-5480:~/Downloads$ sudo  cp -r ca.crt /etc/docker/certs.d/172.16.1.99/


    SEARCH:
      superuserConfig:
        type: REGISTRANT
        password: __RANDOM
      defaultPermGrants: true
      permGrants:
        - resource:
            - type: "cluster"
              value: "kafka-cluster"
          grants:
            - actions: ["CLUSTERACTION"]
              users: [__SU]


    KEYWORD_MAPPER.put(V1Constants.CLUSTER, V2Constants.CLUSTER);
mvn clean install -DskipTests -Pdocker

transwarp@transwarp-Latitude-5480:~/Downloads$ docker tag 2e4 node206:5000/transwarp/guardian:guardian-3.2F
transwarp@transwarp-Latitude-5480:~/Downloads$ docker push node206:5000/transwarp/guardian:guardian-3.2

 1976  vim /lib/systemd/system/docker.service
 1977  sudo vim /lib/systemd/system/docker.service
 1978  systemctl daemon-reload
 1979  sudo systemctl restart docker
 1980  sudo vi /etc/hosts
 1981  docker push node206:5000/transwarp/guardian:guardian-3.2

transwarp@transwarp-Latitude-5480:~/Downloads$ docker push node206:5000/transwarp/guardian:guardian-3.2
The push refers to repository [node206:5000/transwarp/guardian]
Get https://node206:5000/v2/: x509: certificate signed by unknown authority
vim  /lib/systemd/system/docker.service
ExecStart后面加上--insecure-registry node206:5000
保存完执行 systemctl darmon-reload
systemctl restart docker


启动不了guardian上的服务->没有keytab->缺少注册用户->manager版本旧
集群 数据库端口8320

2020-01-06 19:33:31,198 ERROR io.transwarp.guardian.server.boot.exception.GuardianExceptionHandler: Exception occurs and handled by GuardianExceptionHandler:
io.transwarp.guardian.common.exception.GuardianException: ErrorCode: 1005, ErrorMessage: User [elasticsearch] doesnt exist in Guardian
        at io.transwarp.guardian.persistence.dao.impl.UserDaoImpl.getUserLockState(UserDaoImpl.java:299)
        at io.transwarp.guardian.core.manager.v2.UserManager.loginSpnego(UserManager.java:223)
        at io.transwarp.guardian.core.manager.v2.UserManager$$FastClassBySpringCGLIB$$c8969549.invoke(<generated>)

ranswarp@transwarp-Latitude-5480:~/Downloads$ docker images
REPOSITORY                            TAG                 IMAGE ID            CREATED             SIZE
transwarp/apacheds                    latest              0e98025e8bd0        18 minutes ago      1.18GB
transwarp/guardian                    latest              2e4dc8b53b4d        18 minutes ago      1.34GB
transwarp/guardian-migration          latest              d10e753dfffe        19 minutes ago      1.13GB
<none>                                <none>              89d7444f82f7        2 hours ago         1.18GB
<none>                                <none>              3979eeef0c16        2 hours ago         1.34GB
172.16.1.99/transwarp/tdh-baseimage   transwarp-6.2       8049f76605fd        5 months ago        1.05GB
hello-world                           latest              fce289e99eb9        12 months ago       1.84kB
transwarp@transwarp-Latitude-5480:~/Downloads$ docker tag 2e4 node206:5000/transwarp/guardian:guardian-3.2
transwarp@transwarp-Latitude-5480:~/Downloads$ docker push node206:5000/transwarp/guardian:guardian-3.2
The push refers to repository [node206:5000/transwarp/guardian]
Get https://node206:5000/v2/: x509: certificate signed by unknown authority

2020-01-06 20:37:49,957 INFO org.apache.directory.fortress.core.util.Config: static init: found from: fortress.properties path: /etc/guardian/conf/fortress.properties
2020-01-06 20:37:49,982 INFO org.apache.directory.fortress.core.util.Config: static init: loading from: fortress.properties
2020-01-06 20:37:55,057 INFO io.transwarp.guardian.plugins.filter.SpnegoAuthFilter: SPNEGO initiated with server principal [guardian/guardian]
2020-01-06 20:37:55,059 INFO io.transwarp.guardian.plugins.filter.SpnegoAuthFilter: SPNEGO completed for client principal [elasticsearch/suse01@TDH]
2020-01-06 20:37:55,063 ERROR io.transwarp.guardian.server.boot.exception.GuardianExceptionHandler: Exception occurs and handled by GuardianExceptionHandler:
io.transwarp.guardian.common.exception.GuardianException: ErrorCode: 1005, ErrorMessage: User [elasticsearch] doesnt exist in Guardian
        at io.transwarp.guardian.persistence.dao.impl.UserDaoImpl.getUserLockState(UserDaoImpl.java:299)

User [] doesnt exist in Guardian

0107
yaml文件格式-写的有问题-ElasticSearch

撤销git commit
git reset --soft HEAD^ 软撤销 保留上次工作空间
or
git revert HEAD ->在当前提交后提交新提交revert


keymapper冲突
search guardian重启

  //for ElasticSearch
  private static final Set<String> SEARCH_RESOURCE = new TreeSet<>(String.CASE_INSENSITIVE_ORDER);

v1转v2
kafka和search都用cluster，不过kafka v2要搜索路径模糊查询是cluster ""
search有cluster * 导致冲突可能要写一个开关？

v2转v1的问题转化过ut
 mvn install -DskipTests -Pdocker


0108

56029=Get SEARCH index error with pattern {0}
56030=Create SEARCH transporter client error
56031=Cannot get columns of table from inceptor
56032=Failed to lookup inceptor resource

  int INCEPTOR_COLUMN_FETCH_ERROR = 56029;
  int INCEPTOR_RESOURCE_LOOKUP_ERROR = 56030;
  int GET_SEARCH_INDEX_ERROR = 56031;
  int CREATE_SEARCH_CLIENT_ERROR = 56032;

${log4j.version}->定义在总xml文件中

https://www.warpcloud.cn/#/documents-support/docs-detail/document/TDH-PLATFORM/6.2/030HyperbaseManual?docType=docs%3Fcategory%3DTDH%26index%3D0&docName=Hyperbase%E4%BD%BF%E7%94%A8%E6%89%8B%E5%86%8C

从物理架构上看，HBase是包含三类服务器的主从式架构。 RegionServer 负责响应用户I/O请求，并向 HDFS 中读写数据。每当访问数据时， 客户端（Client） 会直接与RegionServer建立连接。region的分配、DDL（创建、删除表）操作则由 HMaster 来处理。 Zookeeper ，它作为HDFS的一部分，负责维护集群的健康状态、避免HMaster单点问题。

Hadoop DataNode 存储有RegionServer正在管理的数据。所有HBase的数据都存储在HDFS文件中。RegionServer和HDFS DataNode并置，这使得由RegionServer处理的数据具有 数据局部性 （ data locality ，数据被放在需要它的地方的附近）。HBase的数据在写入的时候可从本地获取，但当它所属的region被移走时，则需要从远端获取数据，直到等到 Compaction 操作。

NameNode为组成文件的物理数据块维护着它们的 元数据（metadata） 信息。

Hbase是Hadoop database，即Hadoop数据库。它是一个适合于非结构化数据存储的数据库，HBase基于列的而不是基于行的模式。
hdfs文件系统

license问题
-----------------------------
查找hyperbase什么时机注册guardian并判断是否应该这样
hmaster
活跃的节点存在Initialize GuardianAuthManager done
020-01-08 12:35:29,782 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initializing GuardianAuthManager ...
2020-01-08 12:35:29,820 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Service hyperbase1 startup username obtained: hbase
2020-01-08 12:35:29,849 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1237:8380
2020-01-08 12:35:29,849 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1238:8380
2020-01-08 12:35:30,498 INFO io.transwarp.guardian.client.GuardianClient: Login guardian client using configuration in guardian-site.xml implicitly
2020-01-08 12:35:30,794 INFO io.transwarp.guardian.client.GuardianClient: Guardian client cache enabled: true
2020-01-08 12:35:30,802 INFO io.transwarp.guardian.client.cache.metrics.PeriodCacheMetricsDisplay: Start PeriodCacheMetricsDisplay, display period: 600000 milliseconds
2020-01-08 12:35:30,868 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Register GuardianCache CheckAccessCache
2020-01-08 12:35:30,868 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Register GuardianCache QuotaCache
2020-01-08 12:35:30,868 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Staring a PeriodCacheUpdater to update caches of Guardian Client
2020-01-08 12:35:31,371 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: The first time to fetch change list, just keep the latest change list version: 0
2020-01-08 12:35:31,372 INFO io.transwarp.guardian.client.GuardianClient: Guardian client heartbeat report enabled: true
2020-01-08 12:35:31,375 INFO io.transwarp.guardian.client.PeriodHeartbeatReporter: Starting a PeriodHeartbeatReporter for [hyperbase1] to report heartbeat to guardian server, report interval: 60000 milliseconds
2020-01-08 12:35:31,376 INFO io.transwarp.guardian.client.GuardianClientFactory: Create a new instance of RestClientImpl
2020-01-08 12:35:31,419 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1237:8380
2020-01-08 12:35:31,419 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1238:8380
2020-01-08 12:35:31,447 INFO io.transwarp.guardian.client.impl.rest.RestAdminImpl: Login guardian admin using configuration in guardian-site.xml implicitly
2020-01-08 12:35:31,453 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Fetch change version: 0
2020-01-08 12:35:31,577 INFO io.transwarp.guardian.client.GuardianAdminFactory: Create a new instance of RestAdminImpl
2020-01-08 12:35:32,631 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initialize GuardianAuthManager done

020-01-08 12:35:21,931 INFO org.apache.hadoop.hbase.util.ServerCommandLine: env:GUARDIAN_PLUGINS_CONF_DIR=/usr/lib/guardian-plugins/templates

log4j问题->fetch rebase
---------------
regionserver
存在Initialize GuardianAuthManager done
2020-01-08 12:45:40,227 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initialize GuardianAuthManager done

Initializing GuardianAuthManager

2020-01-08 12:45:37,575 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initializing GuardianAuthManager ...
2020-01-08 12:45:37,621 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Service hyperbase1 startup username obtained: hbase
2020-01-08 12:45:37,644 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1237:8380
2020-01-08 12:45:37,644 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1238:8380
2020-01-08 12:45:38,166 INFO io.transwarp.guardian.client.GuardianClient: Login guardian client using configuration in guardian-site.xml implicitly
2020-01-08 12:45:38,621 INFO io.transwarp.guardian.client.GuardianClient: Guardian client cache enabled: true
2020-01-08 12:45:38,627 INFO io.transwarp.guardian.client.cache.metrics.PeriodCacheMetricsDisplay: Start PeriodCacheMetricsDisplay, display period: 600000 milliseconds
2020-01-08 12:45:38,671 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Register GuardianCache CheckAccessCache
2020-01-08 12:45:38,671 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Register GuardianCache QuotaCache
2020-01-08 12:45:38,671 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Staring a PeriodCacheUpdater to update caches of Guardian Client
2020-01-08 12:45:39,092 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: The first time to fetch change list, just keep the latest change list version: 0
2020-01-08 12:45:39,094 INFO io.transwarp.guardian.client.GuardianClient: Guardian client heartbeat report enabled: true
2020-01-08 12:45:39,129 INFO io.transwarp.guardian.client.PeriodHeartbeatReporter: Starting a PeriodHeartbeatReporter for [hyperbase1] to report heartbeat to guardian server, report interval: 60000 milliseconds
2020-01-08 12:45:39,129 INFO io.transwarp.guardian.client.GuardianClientFactory: Create a new instance of RestClientImpl
2020-01-08 12:45:39,169 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1237:8380
2020-01-08 12:45:39,169 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1238:8380
2020-01-08 12:45:39,188 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Fetch change version: 0
2020-01-08 12:45:39,201 INFO io.transwarp.guardian.client.impl.rest.RestAdminImpl: Login guardian admin using configuration in guardian-site.xml implicitly
2020-01-08 12:45:39,420 INFO io.transwarp.guardian.client.GuardianAdminFactory: Create a new instance of RestAdminImpl
2020-01-08 12:45:40,227 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initialize GuardianAuthManager done


hyperbase组件
hyperbase-master-hyperbase1-5cb5cfcb4-dwt9f          1/1       Running            0          56m       172.16.1.238   tw-node1238
hyperbase-master-hyperbase1-5cb5cfcb4-gzctk          0/1       CrashLoopBackOff   15         56m       172.16.1.236   tw-node1236
hyperbase-master-hyperbase1-5cb5cfcb4-xhmbk          1/1       Running            0          56m       172.16.1.237   tw-node1237
hyperbase-regionserver-hyperbase1-6684d5f8b7-2lbr6   1/1       Running            0          45m       172.16.1.237   tw-node1237
hyperbase-regionserver-hyperbase1-6684d5f8b7-p6n8c   1/1       Running            0          45m       172.16.1.238   tw-node1238
hyperbase-regionserver-hyperbase1-6684d5f8b7-xxl6j   0/1       CrashLoopBackOff   13         45m       172.16.1.236   tw-node1236
hyperbase-thrift-hyperbase1-6c46b56d5-trprb          1/1       Running            0          35m       172.16.1.237   tw-node1237

每个RegionServer都运行在HDFS的一个数据节点上，RegionServer由如下部分构成：

WAL : Write Ahead Log是分布式文件系统上的文件。WAL保存着一些临时的新数据，以用作之后故障的修复。

BlockCache : BlockCache是读操作时的cache，它保存着内存中经常被读的数据。当cache填满后， 最近最少（Least Recently Used） 使用的数据将被剔除。

MemStore : MemStore是写操作时的cache。它保存着尚未写入磁盘的新数据，这些数据在被写入磁盘前是有序的。每个region的每一 列族（column family） 都有一个MemStore。

Hfiles ： Hfiles保存着磁盘上有序 键值对（KeyValues） 的行信息。

落后提交->git rebase -i HEAD~3
r

只有regionserver会出现register日志，所以猜想只有regionserver才会连guardian
然后hbase-server也出现了一据acl认证权限的逻辑

hbase-client org.apache.hadoop.hbase.security.access;
ACL表在zk上
1.AccessController实现了CoprocessorService、AccessControlService.Interface，通过Java或者命令行执行grant、revoke操作时，会相应的调用AccessController的grant、revoke方法，方法中会将配置的权限存进hbase:acl中。
2. Coprocessor提供了在各个操作之前和之后的回调，相应的可以从方法名中看出，例如：preScannerOpen、postScannerOpen。AccessController继承了Coprocessor，在操作前回调pre里会调用AccessController的permissionGranted方法来判断是否有权限执行permRequest这个Action。
3.权限是从配置文件和hbase:acl表中读取出来的，都存在TableAuthManager中，分为globalCache、nsCache和tableCache。
4.更新缓存同步的其实是hbase:acl表中的数据，配置文件不会更新。之前提到在start中，将自己注册到watcher(ZooKeeperWatcher)中，本质是实现ZooKeeperListener的监听。

hbase的权限存储在zk的acl表上，authorize走的是TableAuthManager，一层一层调最后根据acl判断权限

guardian逻辑一样，authorize走到guardianauthManager->guardianClient->rest->guardianController

hbase-server -> accesscontroller -> tableauthManager

regionserverservice/masterservices

hbase用户信息
HBase会选择启动HBase的系统用户作为超级用户
默认用户 分3种情况来获取用户。有KERBEROS，取KERBEROS的用户信息；有HADOOP_USER_NAME，取HADOOP_USER_NAME的用户信息；都没有，就取Unix/Linux系统的用户信息，就是第3步commit到subject中的用户信息
simple用户权限验证
Base提供了AccessController作为自带的认证方式，HBase称之为simple。
hbase --config /etc/hyperbase1/conf shell 进入hyperbaseshell
hyperbase-master-hyperbase1-5cb5cfcb4-dwt9f
2020-01-10 10:41:30,793 ERROR zookeeper.ZooKeeperWatcher: hconnection-0x441b8382-0x626f69ad945e4fb9, quorum=tw-node1237:2181,tw-node1238:2181,tw-node1236:2181, baseZNode=/hyperbase1 Received unexpected KeeperException, re-throwing exception
执行
export HBASE_OPTS="-Djava.security.auth.login.config=/etc/hyperbase1/conf/jaas.conf"
kinit
进入hbase shell
regionserver
19:57:15,725 INFO org.apache.hadoop.hbase.regionserver.RSRpcServices: Open test,,1578484623162.b3a29b193b0ee88a390b8cdf96c6bfee.
2020-01-08 19:57:15,734 WARN org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory: error creating DomainSocketF
java.net.ConnectException: connect(2) error: Permission denied when trying to connect to '/var/run/hdfs1/dn_socket'
        at org.apache.hadoop.net.unix.DomainSocket.connect0(Native Method)
        at org.apache.hadoop.net.unix.DomainSocket.connect(DomainSocket.java:250)
        at org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory.createSocket(DomainSocketFactory.java:175)
        at org.apache.hadoop.hdfs.BlockReaderFactory.nextDomainPeer(BlockReaderFactory.java:753)
        at org.apache.hadoop.hdfs.BlockReaderFactory.createShortCircuitReplicaInfo(BlockReaderFactory.java:471)
        at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.create(ShortCircuitCache.java:782)
        at org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache.fetchOrCreate(ShortCircuitCache.java:716)
        at org.apache.hadoop.hdfs.BlockReaderFactory.getBlockReaderLocal(BlockReaderFactory.java:423)

hmaster
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby
        at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:87)
        at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1785)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1409)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease(FSNamesystem.java:4502)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewLease(NameNodeRpcServer.java:989)
2020-01-08 20:01:10,479 INFO org.apache.hadoop.io.retry.RetryInvocationHandler: Exception while invoking getListing of class ClientNamenodeProtocolTranslatorPB over tw-node1238/172.16.1.238:8020 after 3 fail over attempts. Trying to fail over after sleeping for 2450ms.
java.net.ConnectException: Call From tw-node1238/172.16.1.238 to tw-node1238:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)



0109

SQL 语句主要可以划分为以下 3 个类别。

DDL（Data Definition Languages）语句：数据定义语言，这些语句定义了不同的数据段、数据库、表、列、索引等数据库对象的定义。常用的语句关键字主要包括 create、drop、alter等。

DML（Data Manipulation Language）语句：数据操纵语句，用于添加、删除、更新和查询数据库记录，并检查数据完整性，常用的语句关键字主要包括 insert、delete、udpate 和select 等。(增添改查）

DCL（Data Control Language）语句：数据控制语句，用于控制不同数据段直接的许可和访问级别的语句。这些语句定义了数据库、表、字段、用户的访问权限和安全级别。主要的语句关键字包括 grant、revoke 等。

meta表->查找region
acl表
Coprocessor
把一部分计算也移动到数据的存放端；允许用户执行region级的操作；可以动态加载
1、使用钩子来关联行修改操作来维护辅助索引，或维护一些数据间的引用完整性。
2.权限控制
coprocessor->observer/endpoint
与触发器类似;

regionobserver处理数据修改事件，表region联系紧密;

MasterObserver集群级事件操作，管理或DDL类型操作;

WALObserver控制WAL。
用户自定义操作添加到服务端添加一些远程过程调用动态拓展RPC协议；与RDBMS存储类似

权限是从配置文件和hbase:acl表中读取出来的，都存在TableAuthManager中，分为globalCache、nsCache和tableCache。
Table可以直接理解为表，而Family和Qualifier其实都可以理解为列，一个Family下面可以有多个Qualifier，所以可以简单的理解为，HBase中的列是二级列，也就是说Family是第一级列，Qualifier是第二级列。两个是父子关系。
每一个family会分配一个memstore

Hbase每时每刻只有一个hmaster主服务器程序在运行，hmaster将region分配给region服务器，协调region服务器的负载并维护集群的状态。Hmaster不会对外提供数据服务，而是由region服务器负责所有regions的读写请求及操作。

　　由于hmaster只维护表和region的元数据，而不参与数据的输入/输出过程，hmaster失效仅仅会导致所有的元数据无法被修改，但表的数据读/写还是可以正常进行的。
可以看到，client访问hbase上的数据并不需要master参与（寻址访问zookeeper和region server，数据读写访问region server），master仅仅维护table和region的元数据信息（table的元数据信息保存在zookeeper上），负载很低。
注意：master上存放的元数据是region的存储位置信息，但是在用户读写数据时，都是先写到region server的WAL日志中，之后由region server负责将其刷新到HFile中，即region中。所以，用户并不直接接触region，无需知道region的位置，所以其并不从master处获得什么位置元数据，而只需要从zookeeper中获取region server的位置元数据，之后便直接和region server通信。

hbase读写
client先访问zk,从meta表中获取相应region信息（这个时候会访问acl表，处理权限），刷入自己的缓存，找到对应的regionServer。请求到对应的regionserver，先把数据写到WAL/hlog和memstore上各一份，memstore负责操作，达到阀值刷新到storefile，flush到hfile，多个hfile达到大小compact合并，请求道Hmaster进行region split。regionserver和hdfs的datanode热拷贝，进行存储。
hmaster启动时会将meta表加载到zk，竞争成为active，定期处理region信息并执行balance更新分配，向regionserver传心跳，处理regionInfo信息协调zk。增删改的DDL操作通过regionserver(用户操作)和zk(加锁)的监听协调后改动region，然后调用connection去put.
权限是从配置文件和hbase:acl表中读取出来的，都存在TableAuthManager中，分为globalCache、nsCache和tableCache。
guardian上会通过guardian server直接判断权限

如何判断使用范围 hbase的accessController都是通过coprpcessor来实现


一致性
Hbase是一个强一致性数据库，不是“最终一致性”数据库，官网给出的介绍：

“Strongly consistent reads/writes: HBase is not an "eventually consistent" DataStore. This makes it very suitable for tasks such as high-speed counter aggregation.”
CP

userpermission->namespace/tablename/family/qualifier username

目标：由于regionserver太多，每个regionserver向guardian注册发送心跳影响性能，改动为只有active master注册
AccessController -> public void start(CoprocessorEnvironment env)

conf文件一般在哪定义
如果prohibit的话直接在conf文件改？->不注册就好


报错：
2020-01-09 15:53:47,597 DEBUG org.apache.hadoop.hbase.ipc.RpcServer: RpcServer.FifoWFPBQ.priority.handler=199,queue=1,port=60020: callId: 276 service: AdminService methodName: OpenRegion size: 82 connection: 172.16.1.237:58924
org.apache.hadoop.hbase.regionserver.RegionServerAbortedException: Server tw-node1237,60020,1578556414501 aborting
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.checkOpen(RSRpcServices.java:1219)
        at org.apache.hadoop.hbase.regionserver.RSRpcServices.openRegion(RSRpcServices.java:1632)


2020-01-09 15:53:47,340 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.GuardianAccessController, org.apache.hadoop.hbase.security.token.TokenProvider, org.apache.hadoop.hyperbase.secondaryindex.coprocessor.Indexer, org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint, org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint]
2020-01-09 15:53:47,341 ERROR org.apache.hadoop.hbase.coprocessor.CoprocessorHost: The coprocessor org.apache.hadoop.hbase.security.access.GuardianAccessController threw java.lang.NullPointerException
java.lang.NullPointerException
        at org.apache.hadoop.hbase.security.access.GuardianAccessController.permissionGranted(GuardianAccessController.java:125)
        at org.apache.hadoop.hbase.security.access.GuardianAccessController.permissionGranted(GuardianAccessController.java:194)
只停register，不停auth
manager找jar包时间最久远的那个

测试过程停掉全部hbase节点 删除guardianhbase服务 启动hbase Regionserver节点 日志无异常 观察服务注册 启动hmaster节点 观察服务注册 看日志
在regionserver处关闭心跳 判断hmaster只有注册

0110
修改最后提交的注释
git commit --amend 
git rebase -i HEAD~^ 把pick改成edit

license问题
zai节点下etc/conf找
  315  kubectl get po -owide
  316  kubectl logs hadoop-hdfs-namenode-hdfs1-7589df47f5-6l2kc
  317  kubectl logs hadoop-hdfs-namenode-hdfs1-7589df47f5-6l2kc -c hadoop-hdfs-namenode-hdfs1
  318  kubectl get po -owide
  319  vim /var/log/zookeeper1/zookeeper.log
  320  kubectl describe po zookeeper-server-zookeeper1-f765fc648-kz5g9
  321  cdf -h
  322  df -h
  323  find / -name msl-site.*
  324  cd /etc/transwarp/conf/msl-site.xml 
  325  cat /etc/transwarp/conf/msl-site.xml 
  326  kubectl get po -owide\
  327  kubectl get po -owide
  328  kubectl get po -owide -w
  329  kubectl get po -owide
  330  kubectl logs zookeeper-server-license-7fbfc544fc-24wqw 
  331  vi /var/log/license/
  332  vi /var/log/license/zookeeper.log
  333  \


  707  vim metainfo.yaml 
  708  grep -r jks .
  709  ssh tw-node1237
  710  vim configuration.yaml 
  711  cat /etc/transwarp-manager/master/db.properties 
  712  mysql -S /var/run/mariadb/transwarp-manager-db.sock -utranswarp -p8iKmHuJwGW
  713  vim /etc/my.cnf
  714  [A
  715  vim /etc/my.cnf
  716  cd /etc/transwarp-manager/master/my.cnf 
  717  vim /etc/transwarp-manager/master/my.cnf 
  718  vim /etc/init.d/transwarp-manager-db 
  719  rpm -qa | grep mari
  720  find /var/lib/transwarp-manager/master/pub/ -name mariadb*
  721  find /var/lib/transwarp-manager/master/pub/ -name "mariadb*"
  722  vim /etc/init.d/transwarp-manager-db 
  723  rpm -qa | grep -i mariadb
  724  rpm -qa | grep -i mysql
  725  vim /etc/init.d/transwarp-manager-db 
  726  vim /etc/transwarp-manager/master/linuxdistros/sles12.conf 
  727  find /usr/lib/transwarp-manager/ -name "dnw*.jar"
  728  clear
  729  cd /etc/conf
  730  history 30


残留问题
1236节点部分服务起不来

2020-01-10 09:59:04,436 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: No master found; retry
2020-01-10 10:08:10,423 DEBUG org.apache.hadoop.hbase.regionserver.HRegionServer: No master found; retry

regionserver
2020-01-10 10:09:53,032 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initializing GuardianAuthManager ...
2020-01-10 10:09:53,070 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Service hyperbase1 startup username obtained: hbase
2020-01-10 10:09:53,074 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initialize GuardianAuthManager done

2020-01-10 10:09:40,663 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initializing GuardianAuthManager ...
2020-01-10 10:09:40,706 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Service hyperbase1 startup username obtained: hbase
2020-01-10 10:09:40,730 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1237:8380
2020-01-10 10:09:40,731 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1238:8380
2020-01-10 10:09:41,120 INFO io.transwarp.guardian.client.GuardianClient: Login guardian client using configuration in guardian-site.xml implicitly
2020-01-10 10:09:41,400 INFO io.transwarp.guardian.client.GuardianClient: Guardian client cache enabled: true
2020-01-10 10:09:41,411 INFO io.transwarp.guardian.client.cache.metrics.PeriodCacheMetricsDisplay: Start PeriodCacheMetricsDisplay, display period: 600000 milliseconds
2020-01-10 10:09:41,475 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Register GuardianCache CheckAccessCache
2020-01-10 10:09:41,475 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Register GuardianCache QuotaCache
2020-01-10 10:09:41,475 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Staring a PeriodCacheUpdater to update caches of Guardian Client
2020-01-10 10:09:41,893 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: The first time to fetch change list, just keep the latest change list version: 0
2020-01-10 10:09:41,894 INFO io.transwarp.guardian.client.GuardianClient: Guardian client heartbeat report enabled: true
2020-01-10 10:09:41,895 INFO io.transwarp.guardian.client.PeriodHeartbeatReporter: Starting a PeriodHeartbeatReporter for [hyperbase1] to report heartbeat to guardian server, report interval: 60000 milliseconds
2020-01-10 10:09:41,895 INFO io.transwarp.guardian.client.GuardianClientFactory: Create a new instance of RestClientImpl
2020-01-10 10:09:41,914 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1237:8380
2020-01-10 10:09:41,914 INFO io.transwarp.guardian.client.impl.rest.AbstractGuardianClient: A guardian server is configured : https://tw-node1238:8380
2020-01-10 10:09:41,938 INFO io.transwarp.guardian.client.impl.rest.RestAdminImpl: Login guardian admin using configuration in guardian-site.xml implicitly
2020-01-10 10:09:41,942 INFO io.transwarp.guardian.client.cache.PeriodCacheUpdater: Fetch change version: 0
2020-01-10 10:09:42,111 INFO io.transwarp.guardian.client.GuardianAdminFactory: Create a new instance of RestAdminImpl
2020-01-10 10:09:42,111 INFO io.transwarp.guardian.plugins.hyperbase.GuardianAuthManager: Initialize GuardianAuthManager done

ava.util.concurrent.TimeoutException
	at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at io.transwarp.guardian.resource.ResourceServiceManager.runTimedTask(ResourceServiceManager.java:420)
	at io.transwarp.guardian.resource.ResourceServiceManager.lookupResource(ResourceServiceManager.java:226)
	at io.transwarp.guardian.server.boot.controller.ResourceServiceController.lookupResource(ResourceServiceController.java:104)
	at io.transwarp.guardian.server.boot.controller.ResourceServiceController$$FastClassBySpringCGLIB$$1f811119.invoke(<generated>)
	at org.springframework.cglib.proxy.MethodProxy.invoke(MethodProxy.java:204)


测试过程停掉全部hbase节点 删除guardian-hbase服务 启动hbase Regionserver节点 日志无异常(等待链接master) 观察服务注册(无改动) 启动hmaster节点 观察服务注册(active master注册服务，register) 看日志
启动hbase shell 
list create drop操作
guardian界面赋权限 
guardian抛错 错误(code: 56008): Timeout to fetch resources for this service in time 15000 MILLISECONDS, you can type the resource URI in following input box or retry to lookup
at java.util.concurrent.FutureTask.get(FutureTask.java:205)
	at io.transwarp.guardian.resource.ResourceServiceManager.runTimedTask(ResourceServiceManager.java:420)
	at io.transwarp.guardian.resource.ResourceServiceManager.lookupResource(ResourceServiceManager.java:226)
	at io.transwarp.guardian.server.boot.controller.ResourceServiceController.lookupResource(ResourceServiceController.java:104)


 PermissionVo perm = new PermissionVo(component, dataSource, action.name());

hbase的权限结构
global / namespace->table->family->qualifier 每个部分分别对应一个authorize函数，最后统一走到最后的checkaccess
之前的审查权限逻辑问题在于走了checkAccess借口，每个perm都要发一次请求走逻辑，如果到qualifier层最多要发5次请求

getInheritResources
getAuthorizedResources

v2
getAuthorizedResources
v1
filterPermissions
findPrincPermissions(user/group/role)

curl wttr.in 查天气




